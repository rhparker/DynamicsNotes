\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont}
\usepackage[shortlabels]{enumitem} 
\newcounter{question}
\setcounter{question}{0}
\usepackage{parskip}
\setlength{\parindent}{0pt}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{ esint }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{range}

\title{Dynamics Notes 1}
\author{Ross Parker}

\begin{document}

\section{Contraction Mappings and Their Consequences}

The Banach Fixed Point Theorem not only guarantees the existence of a unique fixed point of a function, but lets us actually compute it by successive approximations. As motivation, consider Newton's Method for root finding.\\

Newton's Method is used to find zeros of a differentiable, real-valed function.

\begin{enumerate}[(i)]
\item Start with an initial guess $x_0$
\item Iterate the algorithm $x_{n+1} = x_n - f(x_n)/f'(x_n)$. This is where the tangent line of $f$ at $(x_n, f(x_n)$ hits the $x-$axis.
\item ????
\item PROFIT
\end{enumerate}

Ideally, this method should converge to a unique zero of $f$, which is hopefully the zero near the initial guess. Define a ``Newton function'' $g$ by

\[
g(x) = x - \frac{f(x)}{f'(x)}
\]

If $x$ is a fixed point of $g$, i.e. $g(x) = x$, then (as long as $f'(x) \neq 0$), $f(x) = 0$. We would like Newton's Method to converge to a fixed point of the Newton function $g$. \\

It turns out that criterion we will need is that the map $g$ is a contraction.\\

\begin{definition}Let $D$ be a subset of a normed vector space $(X, |\cdot|)$. Then $F: D \rightarrow D$ is a \emph{contraction} if there exists a constant $L$ with $0 < L < 1$ such that 
\[
|F(x) - F(y)| \leq L |x - y|
\]
for all $x, y \in D$. In other words, $F$ is Lipschitz with Lipschitz constant $L < 1$.
\end{definition}

We can now state the Banach Fixed Point Theorem, which is also called the Contraction Mapping Principle.\\

% Banach Fixed Point Theorem

\begin{theorem}[Banach Fixed Point Theorem] Let $(X, |\cdot|)$ be a Banach space, and $D \subset X$ a closed, nonempty subset of $X$. Suppose that the map $F: D \rightarrow D$ is a contraction, i.e. there exists a positive constant $L < 1$ such that for all $u, v \in D$,

\[
|F(u) - F(v)| \leq L|u - v|
\]

Then $F$ has a unique fixed point $u^*$ in $D$, i.e. there exists a unique $u^* \in D$ such that $F(u^*) = u^*$.

\begin{proof}
As with many proofs involving Banach spaces, the idea is to construct a Cauchy sequence in $D$ which must then converge since $X$ is complete. Then since $D$ is closed, the limit must be in $D$. Let's do it!

\begin{enumerate}
	\item Start with an arbitrary $u_0 \in D$, then keep hitting this with $F$. For all $n \geq 1$, define $u_n = F(u_{n-1})$. Since $F: D \rightarrow D$, $u_n \in D$ for all $n$.
	\item Since $F$ is a contraction, we have
	\begin{align*}
	|F(u_1) - F(u_0)| &\leq L |u_1 - u_0| \\
	|F(u_2) - F(u_1)| &\leq L |u_2 - u_1| = L|F(u_1) - F(u_0)| \leq L^2 |u_1 - u_0|
	\end{align*}
	Repeating this, we get
	\[
	|F(u_{n}) - F(u_{n-1})| \leq L^n |u_1 - u_0|
	\]
	\item Show $\{ u_n \}$ is a Cauchy sequence. For arbitrary $n, k \geq 1$, using the triangle inequality, we have
	\begin{align*}
	|F(u_{n+k}) - F(u_n)| &\leq \sum_{j=0}^{k-1} |F(u_{n+j+1}) - F(u_{n+j})| \\
	&\leq \sum_{j=0}^{k-1} L^{n+j}|F(u_1) - F(u_0)| \\
	&= L^n |F(u_1) - F(u_0)| \sum_{j=0}^{k-1} L^j \\
	&\leq L^n |F(u_1) - F(u_0)| \sum_{j=0}^{\infty} L^j \\
	&= |F(u_1) - F(u_0)| \frac{L^n}{1 - L} \\
	&\rightarrow 0 \text{ as }n \rightarrow \infty
	\end{align*}
	where, since $0 < L < 1$, we used the sum of the infinite geometric series.
	\item Show $\{ u_n \}$ converges to an element of $D$. Since $\{ u_n \}$ is a Cauchy sequence and $X$ is complete, $u_n$ converges to some element $u^*$ in $X$. Since $D$ is closed and all the $u_n \in D$, $u^* \in D$.
	\item Show $u^*$ is a fixed point of $F$. Since $F$ is Lipschitz, it is continuous, thus by the definition of $u_n$
	\[
	F(u^*) = F(\lim_{n\rightarrow \infty} u_n) = \lim_{n \rightarrow \infty} F(u_n) 
	= \lim_{n \rightarrow \infty} u_{n+1} = u^*
	\]
	and so $F(u^*) = u^*$.
	\item Show $u^*$ is the unique fixed point for $F$ in $D$. We do the usual thing, which is to suppose there are two and take the difference. If $\tilde{u}^*$ is another fixed point, then
	\begin{align*}
	|u^* - \tilde{u}^*| = |F(u^*) - F(\tilde{u}^*)| < L|u^* - \tilde{u}^*|,
	\end{align*}
	which is impossible since $L < 1$.
\end{enumerate}
\end{proof}
\end{theorem}

As a consequence of this, we will prove the inverse function theorem. For motivation, we will look at the one-dimensional case. Suppose $f: \R \rightarrow \R$ is continuously differentiable at $x = a$. Then we know from calculus that if $f'(a) \neq 0$, $f$ is invertible in a neigborhood of $a$. This makes sense, since if $f'$ is continuous and $f'(a) \neq 0$, $f$ is strictly increasing or strictly decreasing in a neighborhood of $a$, and so we can read the inverse off of the graph of $f$ near $a$. Using the chain rule on $f^{-1}(f(x)) = x$ at $x = a$, if $b = f(a)$, then

\[
(f^{-1})'(b) = \frac{1}{f'(a)}
\]

We would like to extend this idea to higher dimensions. We can actually prove this as a corollary of the Implicit Function Theorem once we have proved the Uniform Contraction Mapping Principle, but we can prove it directly here.\\

% inverse function theorem

\begin{theorem}[Inverse Function Theorem]
Let $F: \R^n \rightarrow \R^n$ be continuously differentiable (i.e. $C^1$), and suppose the Jacobian (total derivative) $DF(x_0)$ is invertible at $x_0$. Then $F$ is invertible in a neighborhood of $x_0$. Precisely,

\begin{enumerate}[(i)]
\item There exist neighborhoods $U$ of $x_0$ and $V$ of $y_0 = F(x_0)$, such that the restriction $F|_U: U \rightarrow V$ is a bijection.
\item The inverse $G: V \rightarrow U$ is also continuously differentiable, and for $y \in V$,
\begin{align}
DG(y) &= DF(G(y))^{-1}
\end{align}
\end{enumerate}

\begin{proof}
\begin{enumerate}

\item Let $J_0 = DF(x_0)$. Then since $J_0^{-1} DF(x)$ is continuous at $x = x_0$ and $J_0^{-1} DF(x_0) = I$, we can find $\delta > 0$ such that

\begin{enumerate}
\item $||I - J_0^{-1} DF(x)|| \leq \frac{1}{2}$
\item $DF(x)$ is nonsingular
\end{enumerate}

for all $x \in B$, where $B = \overline{B}(x_0, \delta)$ is a closed ball. (ii) follows from the continuity of the determinant.

\item For any $y \in R^N$, define the map
\begin{equation}
N(x; y) = x - J_0^{-1}(F(x) - y)
\end{equation}
Note that since $J_0$ is nonsingular, $x$ is a fixed point of $N(\cdot; y)$ if and only if $y = F(x)$.

\item Show $N(\cdot; y)$ is contraction on $B$. For all $x \in B$, 
\[
||DN(x; y)|| = ||I - J_0^{-1} DF(x)|| \leq \frac{1}{2}
\]

By the mean value inequality in $\R^n$ (this requires $B$ to be convex, which it of course is), for all $x_1, x_2 \in B$

\[
|N(x_2; y) - N(x_1; y)| \leq \sup_{x \in B} ||DN(x; y)|| \: |x_2 - x_1| \leq \frac{1}{2}|x_2 - x_1|
\]

In other words, on the closed ball $B$, $N$ is Lipschitz with Lipschitz constant 1/2, thus a contraction. 

\item Show for $y$ close to $y_0$ that $N: B \rightarrow B$. Let $V = B(y_0, \epsilon)$ be an open ball, where

\[
\epsilon = \frac{\delta}{ 2 ||J_0^{-1}||}
\]

Then for $x \in B$ and $y \in V$, using the triangle inequality

\begin{align*}
|N(x; y) - x_0| &= |N(x; y) - N(x_0; y) + N(x_0; y) - x_0| \\
&\leq |N(x; y) - N(x_0; y)| + |x_0 - J_0^{-1}(F(x_0) - y) - x_0| \\
&\leq |N(x; y) - N(x_0; y)| + |J_0^{-1}(y - y_0)| \\
&\leq \frac{\delta}{2} + |J_0^{-1}||y - y_0| \\
&\leq \frac{\delta}{2} + ||J_0^{-1}||\frac{\delta}{ 2 ||J_0^{-1}||} = \delta 
\end{align*}

Thus for $y \in V$, $N: B \rightarrow B$.

\item Since $B$ is closed, $\R^n$ is complete, and $N(\cdot; y): B \rightarrow B$ is a contraction for each $y \in V$, by the Banach Fixed Point Theorem, there is a unique $x \in B$ such that $N(x; y) = x$, i.e. a unique $x \in B$ such that $y = f(x)$. Let $U = F^{-1}(V) \subset B$. Since $F$ is continuous, $U$ is open. Then define the inverse map $G: V \rightarrow U$ by $G(y) = x$, where $x$ is the unique fixed point of $N(\cdot; y)$.

\item Show the inverse map $G$ is continuous. This is just one of the many ways we can do this. Let $y_1, y_2 \in V$ and let $x_1 = G(y_1), x_2 = G(y_2)$. Then we note that

\begin{align*}
|N(x_1; y) - N(x_2; y)| &= |(x_1 - J_0^{-1}(F(x_1) - y)) - (x_2 - J_0^{-1}(F(x_2) - y))| \\
&= |(x_1 - x_2) - J_0^{-1} (F(x_1) - F(x_2))|
\end{align*}

Thus since $N(\cdot; y)$ is a contraction, we have

\begin{align*}
|x_1 - x_2| - |J_0^{-1} (F(x_1) - F(x_2))| &\leq |(x_1 - x_2) - J_0^{-1} (F(x_1) - F(x_2))| \\
&= |N(x_1; y) - N(x_2; y)| \\
&\leq \frac{1}{2}|x_1 - x_2|
\end{align*}

Rearrange to get 

\begin{align*}
\frac{1}{2}|x_1 - x_2| &\leq |J_0^{-1} (F(x_1) - F(x_2))| \\
|x_1 - x_2| &\leq 2 |||J_0^{-1}|| \: |F(x_1) - F(x_2)|
\end{align*}

Substituting in $x_1 = G(y_1), x_2 = G(y_2)$ and noting that $G$ is the inverse of $F$, this becomes

\begin{align}\label{Glip}
|G(y_1) - G(y_2)| &\leq 2 |||J_0^{-1}|| \: |y_1 - y_2|
\end{align}

This implies $G$ is Lipschitz, thus continuous.

\item Finally, we show that $G$ is differentiable for $y \in V$, and that $DG(f(x)) = DF(x)^{-1}$ for $x \in U$. There are many equivalent definitions of differentiability, but we use the following. Since $F: \R^n \rightarrow \R^n$ is differentiable at $x \in U$, we can write $F$ near $x$ as

\begin{equation}\label{diffF}
F(x + h) = F(x) + DF(x)h + \epsilon(h)
\end{equation}

where $DF(x)$ is a linear transformation ($n \times n$ matrix), and $|\epsilon(h)|/|h| \rightarrow 0$ as $h \rightarrow 0$. Since $U \in B$, $DF(x)$ is invertible for all $x \in U$. \\

Let $y \in V$. Since $V$ is open, take $k$ sufficiently small so that $y + k \in V$ (and it remains in $V$ as we take $k \rightarrow 0$). Let $x = G(y)$, $x + h = G(y + k)$. Since $G: V \rightarrow U$, $x, x+h \in U$. Since $F$ is differentiable at $x$, we substitute these into \eqref{diffF} to get

\[
y + k = y + DF(G(y))(G(y + k) - G(y)) + \epsilon(h)
\]

where $|\epsilon(h)|/|h| \rightarrow 0$ as $h \rightarrow 0$. Rearranging and multiplying by $DF(G(y))^{-1}$, we have

\[
G(y + k) = G(y) + DF(G(y))^{-1}k - DF(G(y))^{-1}\epsilon(h)
\]

All that remains is to show that $|DF(G(y))^{-1}\epsilon(h)|/|k| \rightarrow 0$ as $k \rightarrow 0$. Note that since $G$ is continuous at $y$ and $h = G(y + k) = G(y)$, $h \rightarrow 0$ as $k \rightarrow 0$. We write this as
 
\begin{align*}
\frac{| DF(G(y))^{-1}\epsilon(h)| }{|k|} 
&\leq || DF(G(y))^{-1}|| \frac{ \epsilon(h)}{|h|}\frac{|h|}{|k|}
\end{align*}

By the Lipschitz continuity of $G$ in \eqref{Glip}, 

\begin{align*}
|h| = |x + h - x| = |G(y + k) - G(y)| \leq 2 ||J_0^{-1}|| \: |y + k - y| = 2 ||J_0^{-1}|| \: |k|
\end{align*}

Substituting this above, we obtain

\begin{align*}
\frac{| DF(G(y))^{-1}\epsilon(h)| }{|k|} 
&\leq 2 ||DF(G(y))^{-1}|| \: ||J_0^{-1}|| \frac{ |\epsilon(h)|}{|h|}\frac{|k|}{|k|} \\
&= 2 ||DF(G(y))^{-1}|| \: ||J_0^{-1}|| \frac{ |\epsilon(h)|}{|h|} \\
&\rightarrow 0 \text{ as }k \rightarrow 0
\end{align*}

\end{enumerate}
\end{proof}
\end{theorem}

We can generalize this with the Uniform Contraction Mapping Principle. The idea is that we have a family of contractions $F(x; \mu)$ indexed by a parameter $\mu$. Let $G(\mu)$ map each $\mu$ to the corresponding unique fixed point of $F(\cdot; \nu)$ from the Banach Fixed Point Theorem. Then we will show that $G$ is as smooth as $F$.

\begin{theorem}[Uniform Contraction Mapping Principle]
Let 
\begin{enumerate}
\item $(X, |\cdot|)$ be a Banach space
\item $D \subset X$ a closed, nonempty subset of $X$
\item $B \subset R^p$ be an open set (the ``parameter'' space)
\item $F: D \times B \rightarrow D$ a map which is \emph{uniform contraction}, i.e. there exists a constant $L < 1$ such that for all $\mu \in B$ and $u, v \in D$
\begin{equation}
|F(u, \mu) - F(v, \mu)| \leq L|u - v|
\end{equation}
\end{enumerate}

Let $G: B \rightarrow D$ be the map which associates every $\mu \in B$ with the unique fixed point of $F(\cdot; \mu)$ from the Banach Fixed Point Theorem. Then

\begin{enumerate}[(i)]
\item If $F$ is uniformly Lipschitz in $\mu$, i.e. if there exists a constant $M > 0$ such that for all $u \in D$ and $\mu_2, \mu_2 \in B$, 
\[
|F(u, \mu_1) - F(u, \mu_2)| \leq M |\mu_1 - \mu_2|
\]
then $G$ is Lipschitz, with Lipschitz constant $M / (1 - L)$.
\item If $F \in C^k(D \times B, X)$ for $k \geq 0$, then $G \in C^k(B, X)$. $k = \infty$ and analyticity are allowed.
\end{enumerate}
\begin{proof}

\begin{enumerate}
\item First, we derive an expression for $|G(\mu_1) - G(\mu_2)|$. Using the fact that $G(\mu)$ is a fixed point of $F(\cdot; \mu)$ and the triangle inequality

\begin{align*}
|G(\mu_1) - G(\mu_2)| &= |F( G(\mu_1); \mu_1) - F(G(\mu_2); \mu_2)| \\
&\leq |F( G(\mu_1); \mu_1) - F(G(\mu_1); \mu_2)| + |F( G(\mu_1); \mu_2) - F(G(\mu_2); \mu_2)| \\
&\leq |F( G(\mu_1); \mu_1) - F(G(\mu_1); \mu_2)| + L |G(\mu_1) - G(\mu_2)|
\end{align*}

Since $0 < L < 1$, subtract the last term on the RHS to get

\begin{align*}
(1 - L)|G(\mu_1) - G(\mu_2)| 
&\leq |F( G(\mu_1); \mu_1) - F(G(\mu_1); \mu_2)|
\end{align*}

Divide by $(1 - L)$ to get 

\begin{align}\label{Gmudiff}
|G(\mu_1) - G(\mu_2)| &\leq \frac{1}{1-L}|F( G(\mu_1); \mu_1) - F(G(\mu_2); \mu_2)|
\end{align}

\item To show (i), if $F$ is uniformly Lipschitz in $\mu$, then \eqref{Gmudiff} becomes 

\begin{align*}
|G(\mu_1) - G(\mu_2)| &\leq \frac{M}{1-L}| G(\mu_1); \mu_1) - G(\mu_1); \mu_2)| \\
&= \frac{M}{1-L} |\mu_1 - \mu_2| \\
\end{align*}

\item For (ii), we first take the case $k = 0$ (i.e. $F$ is continuous in both variables). Since the RHS of \eqref{Gmudiff} $\rightarrow 0$ as $\mu_2 \rightarrow \mu_1$, $G$ is continuous.

\item Next, we consider the case $k = 1$. Recall that $G(\mu) = F(G(\mu),\mu)$. If $G$ were differentiable, then by the chain rule we would have

\[
DG(\mu) = DF(G(\mu),\mu) = D_X F(G(\mu), \mu)DG(\mu) + D_B F(G(\mu), \mu)
\]

In other words, $DG(\mu)$ is a fixed point of the mapping $\Phi: \mathcal{L}(B, X) \times B \rightarrow \mathcal{L}(B, X)$

\[
\Phi(A; \mu) = D_X F(G(\mu), \mu) A + D_B F(G(\mu), \mu)
\]

The map $\Phi$ is a uniform contraction, since for $A_1, A_2 \in \mathcal{L}(B, X)$

\begin{align*}
|\Phi(&A_1; \mu) - \Phi(A_2; \mu)| \\
&= | D_X F(G(\mu), \mu) A_1 + D_B F(G(\mu), \mu) - D_X F(G(\mu), \mu) A_2 + D_B F(G(\mu), \mu) | \\
&\leq | D_X F(G(\mu), \mu) |\:|A_1 - A_2| \\
&\leq L |A_1 - A_2|
\end{align*}

where the last line holds since $F(\cdot; \mu)$ is Lipschitz with constant $L$ (this follows from the definition of the derivative and is not hard to show).\\

By the Uniform Contraction Mapping Principle, for each $\mu \in D$ there a unique fixed point $Z(\mu)$ of $\Phi$. Since $F$ is $C^1$, $\Phi$ is continuous, thus the map $Z(\mu)$ is continuous.\\

The only thing that remains is to show that $DG(\mu)$ exists and is equal to $Z(\mu)$. This is rather technical, so it will be omitted for now. (References will be provided at some point.)

\item 

\end{enumerate}

\end{proof}
\end{theorem}

We will use the uniform contraction mapping principle to prove the implicit function theorem, which is one of the most important analytical tools in dynamcial systems.\\

Before we do that, here is a motivating example.\\

Consider the unit circle, $x^2 + y^2 = 1$. This is clearly not a function, since other than the right and left endpoints $(0, 1)$ and $(0, -1)$, there are two values of $y$ for every $x$. That being said, at any other point, we can solve \emph{locally} for $y$ as a function of $x$. Near any point on the upper semicircle, for example, we have $y = \sqrt{1 - x^2}$. \\

Why does this fail at the left and right endpoints? Looking at the picture, it is clear that we cannot solve for $y$ in terms of $x$ there. For a more mathematical explanation, let's write the unit circle as the zero set of the function $f(x, y) = x^2 + y^2 - 1$. At $(0, 1)$, we have $\partial f / \partial y = 0$. In other words, near $(0, 1)$, $f$

We can now state and prove the implicit function theorem.

\begin{theorem} 
Let
\begin{enumerate}
\item $X$, $Y$, $Z$ Banach spaces
\item $U \subset X$, $V \subset Y$ open sets
\item $F: U \times V \rightarrow Z$ continuously differentiable
\item $(x_0, y_0) \in X \times Y$ with $F(x_0, y_0) = 0$
\item The total (Frechet) derivative $D_x(x_0, y_0): X \rightarrow Z$ has a bounded inverse
\end{enumerate}
Then
\begin{enumerate}
\item We can solve for $x$ as a function of $y$ near $(x_0, y_0)$. That is, there is a neighborhood $U_0 \times V_0 \subset U \times V$ of $(x_0, y_0)$ and a continuously differentiable function $f: V_0 \rightarrow U_0$ with $f(y_0) = x_0$ such that $F(x, y) = 0$ for $(x, y) \in U_0 \times V_0$ if and only if $x = f(y)$.

\item If $F$ is $C^k(U \times V, Z)$ (or analytic) in a neighborhood of $(x_0, y_0)$, then $f is C^k(V_0,X)$ (or analytic) in a neighborhood of $y_0$.
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item Since $D_x(x_0, y_0)$ is invertible, define $L: Z \rightarrow X$ by $Lz = [D_x(x_0, y_0)]^{-1}z$. 
\item Define the map $G: U \times V \rightarrow X$ by 
\begin{equation}\label{defG}
G(x, y) = x - L F(x, y)
\end{equation}
Since $F$ is $C^1$, $G$ is $C^1$ as well, and

\[
DG_x(x, y) = I - L \: DF_x(x, y)
\]

In particular, $DG_x(x_0, y_0) = 0$.

\item $F(x, y) = 0$ if an only if $G(x, y) = x$, i.e. $x$ is a fixed point of $G$. We will show that $G$ is a uniform contraction on a neighborhood of $(x_0, y_0)$. 

\item Since $DG_x(x, y)$ is continuous and $DG_x(x_0, y_0) = 0$, we can find open balls $U_0 = B(x_0, \delta)$ and $V_0 = B(y_0, \epsilon)$ such that for $(x, y) \in \overline{U_0} \times V_0$,

\[
||DG_x(x, y)|| < \frac{1}{2} 
\]

\item Since the map $y \rightarrow F(x_0, y)$ is continuous in $y$ and $F(x_0, y_0) = 0$, we can (if needed) decrease $\epsilon$ so that for $y \in V_0$

\[
|F(x_0, y)| \leq \frac{\delta}{2 ||L||}
\]

\item Next, we show that $G: \overline{U_0} \times V_0 \rightarrow \overline{U_0}$. By the triangle inequalty and the mean value inequality, for $x \in \overline{U_0}$

\begin{align*}
|G(x, y) - x_0| &\leq |G(x, y) - G(x_0, y)| + |G(x_0, y) - x_0| \\
&\leq \sup_{x\in \overline{U_0}}||DG_x(x, y)||\:|x - x_0| + |x_0 - L F(x_0, y) - x_0 | \\
&\leq \frac{1}{2} |x - x_0| + |L F(x_0, y)| \\
&\leq \frac{\delta}{2} + ||L|| \frac{\delta}{2 ||L||} \\
&\leq \delta
\end{align*}

Thus we have shown that $G: \overline{U_0} \times V_0 \rightarrow \overline{U_0}$. 

\item Finally, we show that $G$ is a uniform contraction. This holds by the mean value inequality, since for $x_1, x_2 \in \overline{U_0}$ and any $y \in V_0$,

\[
|G(x_1, y) - G(x_2, y)| \leq \sup_{x\in \overline{U_0}}||DG_x(x, y)||\:|x_1 - x_2| \leq \frac{1}{2}|x_1 - x_2|
\]
\item Thus, by the Uniform Contraction Mapping Principle, there is a unique $C^1$ function $f: V_0 \rightarrow U_0$ which maps $y \in V_0$ to the unique fixed point $x$ of $G(\cdot, y)$. Since $F(x, y) = 0$ if and only if $x$ is a fixed point of $G(\cdot, y)$, this implies that 
	\begin{enumerate}
	\item $f(y_0) = x_0$
	\item For $(x, y) \in U_0 \times V_0$, $F(x, y) = 0$ if and only if $x = f(y)$
	\end{enumerate}
\end{enumerate}
\end{proof}
\end{theorem}

\section{Existence and Uniqueness}

Consider the initial value problem

\begin{align}\label{IVP}
\frac{du}{dt} &= f(u) \\
u(0) &= u_0 \nonumber
\end{align}

where $f: \R^n \rightarrow \R^n$ is (at minimum) continuous.\\

We would like to answer the following questions:
\begin{enumerate}
	\item Does the IVP \eqref{IVP} have a solution? In other words, can we find a time interval $[a, b]$ (containing 0) and a differentiable function $u: [a, b] \rightarrow \R^n$ for which \eqref{IVP} is satisfied for all $t \in (a, b)$.
	\item Is that solution unique?
\end{enumerate}

Before we delve into the theory, let's look at some simple examples on $\R$.

\begin{enumerate}

\item Exponential growth/decay
\begin{align*}
\frac{du}{dt} &= k u \\
u(0) &= u_0 
\end{align*}
By separation of variables, this has solution
\[
u(t) = u_0 e^{kt}
\]
which exists for all time $t$.

\item ``Superexponential'' growth
\begin{align*}
\frac{du}{dt} &= u^2 \\
u(0) &= u_0
\end{align*}
By separation of variables, this has solution
\[
u(t) = \frac{u_0}{1 - u_0 t}
\]
If we take $u_0 = 1$, the solution becomes
\[
u(t) = \frac{1}{1 - t}
\]
Since we start at $t = 0$, $u(t) \rightarrow \infty$ as $t \rightarrow 1$. In other words, the solution blows up in finite time.

\item ``Stopping'' at a boundary
\begin{align*}
\frac{du}{dt} &= -\frac{t}{u+1} \\
u(0) &= 0
\end{align*}
By separation of variable, the solution is
\[
u(t) = \sqrt{1 - t^2} - 1
\]
This solution is finite for all $t \in [-1, 1]$, but it ceases to exist (i.e. acquires a nonzero imaginary part) outside of this interval. As $t \rightarrow \pm 1$ from the inside, $u(t) \rightarrow -1$, which is exactly where the RHS of the IVP has a discontinuity.

\item Nonuniqueness
\begin{align*}
\frac{du}{dt} &= u^{1/3} \\
u(0) &= 0
\end{align*}
We can see by inspection that $u(t) = 0$ is a solution. By separation of variable, we can also find another solution.
\begin{align*}
u(t) &= \left( \frac{2t}{3} \right)^{3/2} && t \geq 0
\end{align*}
We actually have an infinite family of solutions, given piecewise by
\[
u(t) = \begin{cases}
0 & t \leq T \\
\left( \frac{2(t-T)}{3} \right)^{3/2} & t > T
\end{cases}
\]
where $T \geq 0$. (This piecewise function is $C^1$ but not smooth).
\end{enumerate}

We are now ready to discuss existence and uniqueness of solutions to the IVP \eqref{IVP}. The most basic result is due to Peano.

% Peano existence theorem

\begin{theorem}
Consider the initial value problem on $\R^n$
\begin{align}
\frac{du}{dt} &= f(u) \\
u(t_0) &= u_0 \nonumber
\end{align}
If $f$ is continuous in a neighborhood of $(t_0, u_0)$, then there exists at least one solution $u(t)$ defined in a neighborhood of t0. There is no guarantee of uniqueness.
\begin{proof}
A sketch of the proof on $\R$ goes as follows.
\begin{enumerate}
	\item Choose an interval $[a, b]$ containing $t_0$.
	\item Construct a sequence of approximate solutions $\{ u_n(t) \}$ using the forward Euler method, where the mesh size $h \rightarrow 0$ as $n \rightarrow \infty$.
	\item Show this sequence $\{ u_n(t) \}$ is uniformly bounded and equicontinuous.
	\item By Arzela-Ascoli, $\{ u_n(t) \}$ must have a subsequence which converges to $u(t)$.
	\item Show $u(t)$ is differentiable and satisfies the IVP. 
\end{enumerate}
A similar idea works for $\R^n$, but the sequence of approximations is less straightforward.
\end{proof}
\end{theorem}

Although this is a useful result, we really want uniqueness. To do that, we will the contraction mapping principle to prove existence and uniqueness in one shot. However, we will require a stronger condition on $f$ than just continuity.

% Picard - Lindelof theorem

\begin{theorem}
Consider the initial value problem on $\R^n$
\begin{align}\label{IVPp}
\frac{du}{dt} &= f(u) \\
u(0) &= u_0 \nonumber
\end{align}
Suppose that $f$ is locally Lipschitz, i.e. for every $\tilde{u} \in \R^n$ there exists a radius $\delta$ and a Lipschitz constant $L$ (both depending on $v$) such that for all $u_1, u_2 \in B(\tilde{u}, \delta)$,
\[
|f(u_1) - f(u_2)| \leq L|u_1 - u_2|
\]
Then for every $\tilde{u} \in \R^n$ there exists a radius $\delta$ and a time interval $[a, b]$ containing 0 such that
\begin{enumerate}[(i)]
\item For each initial condition $u_0 \in B(\tilde{u}, \delta)$ the IVP \eqref{IVPp} has a unique solution $u(t; u_0)$ on $[a, b]$.
\item The map $u_0 \rightarrow u(\cdot; u_0)$ is Lipschitz in $u_0$.
\item If $f$ is $C^k$ for $k \geq 1$, then
\begin{enumerate}
\item The solution $u(t; u_0)$ is $C^{k+1}$ in $t$
\item The map $u_0 \rightarrow u(\cdot; u_0)$ is $C^k$ in $u_0$
\end{enumerate}
$k = \infty$ and analyticity are allowed.
\end{enumerate}
\begin{proof}
The essence of the proof is that we will write the IVP in integrated form, use the integrated form to construct a linear operator between Banach spaces, and then show that this operator is a uniform contraction.
\begin{enumerate}
\item First, we reformulate the problem as an integral equation. By the fundamental theorem of calculus, if $u$ is differentiable, then
\begin{equation*}
u(t) = u(0) + \int_0^t u'(\tau) d \tau
\end{equation*}

Thus $u(t)$ is a solution to the IVP \eqref{IVPp} if an only if it is a solution to the integral equation

\begin{equation}\label{intform}
u(t) = u(0) + \int_0^t f(u(\tau)) d \tau
\end{equation}

\item Next, we define some constants. Choose any $\tilde{u} \in \R^n$.
Since $f$ is locally Lipschitz, we can find a radius $\delta > 0$ and a constant $L > 0$ such that for $u_1, u_2 \in B_{2 \delta}(\tilde{u})$,

\begin{align*}
|f(u_1) - f(u_2)| \leq L |u_1 - u_2|
\end{align*}

Since $f$ is continuous, let

\[
C = \sup \{ f(u) : u \in B_{2 \delta}(\tilde{u}) \}
\]

Finally, let

\[
B = B_\delta(\tilde{u})
\]

\item Now we define our Banach spaces. Consider the closed interval $[-r, r]$, where $r > 0$ is small and will be chosen later. We will show that our unique solution exists for $t \in [-r, r]$. Let

\begin{align*}
X = C^0([-r, r], \R^n)
\end{align*}

Equipped with the supremum norm, $X$ is a Banach space. Let $k(t) \in X$ be the constant function $\tilde{u}$, i.e.

\begin{align*}
k(t) &= \tilde{u} && t \in [-r, r]
\end{align*}

Finally, let $D$ be the closed ball in $X$ of radius $2 \delta$ about $k(t)$, i.e.

\begin{align*}
D = \{ u \in X : \sup_{t \in [-r,r]} | u(t) - \tilde{u} | \leq 2 \delta \}
\end{align*}

\item Next, we define our mapping. Define $F: D \times B \rightarrow X$ by

\begin{align}\label{defF}
[F(u, u_0)](t) = u_0 + \int_0^t f(u(\tau)) d \tau
\end{align}

As we noted above, a fixed point of $F$ is a solution to the IVP \eqref{IVPp}. Since $f$ is at least continuous, it is clear that the RHS of \eqref{defF} is in $X$.

\item Show that for sufficiently small $r$, $F: D \times B \rightarrow D$, i.e. the RHS of \eqref{defF} is actually in $D$. For $u \in D$, $u_0 \in B$,

\begin{align*}
\sup_{|t| \leq r} |F(u, u_0) - \tilde{u}| &= 
\sup_{|t| \leq r} \left|u_0 + \int_0^t f(u(\tau)) d\tau - \tilde{u}\right| \\
&\leq |u_0 - \tilde{u}| - \sup_{|t| \leq r} \int_0^t |f(u(\tau))| d \tau \\
\end{align*}

Since $u \in D$, $u(\tau) \in B_{2 \delta}(\tilde{u})$ for all $\tau \in [-r, r]$, thus $|f(u(\tau)| \leq C$ for all $\tau \in [-r, r]$. Using this and the fact that , we have

\begin{align*}
\sup_{|t| \leq r} |F(u, u_0) - \tilde{u}|
&\leq \delta - \sup_{|t| \leq r} \int_0^t |f(u(\tau))| d \tau \\
\end{align*}





\end{enumerate}
\end{proof}
\end{theorem}

\end{document}