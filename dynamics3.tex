\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont}
\usepackage[shortlabels]{enumitem} 
\newcounter{question}
\setcounter{question}{0}
\usepackage{parskip}
\setlength{\parindent}{0pt}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{ esint }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{range}

\title{Dynamics Notes 3}
\author{Ross Parker}

\begin{document}

\section{Linear Systems}

Consider the ODE

\[
\dot{u} = f(u)
\]

with $u \in \R^n$ and $f \in C^2$, and suppose $u^*(t)$ is a solution. Let $u(t) = u^*(t) + v(t)$ be a perturbation of $u^*(t)$, where (hopefully) $v(t)$ is small. Then $u(t)$ satisfies the ODE if and only if the perturbation $v(t)$ satisfies

\begin{align*}
\dot{v}(t) &= \dot{u}(t) - \dot{u}^*(t) \\
&= f(u(t)) - f(u^*(t)) \\
&= f(u^*(t) + v(t)) - f(u^*(t)) \\
&= f(u^*(t)) + DF(u^*(t))v(t) + \mathcal{O}|v(t)|^2 - f(u^*(t)) \\
&= DF(u^*(t))v(t) + \mathcal{O}|v(t)|^2 
\end{align*}

where we used the trusty Taylor theorem on $f$. Thus, assuming $v(t)$ is small, this system is approximately the linear system

\begin{align*}
\dot{v}(t) &= DF(u^*(t))v(t) 
\end{align*}

which is the linearization of the original system about the solution $u^*(t)$. 

\subsection{Global existence for linear systems}

Thus it is useful to consider systems of the form

\[
\dot{u} = A(t) u
\]

where $u \in \R^n$ and $A:\R \rightarrow \R^{n \times n}$ is continuous. What we would like to do is show that solutions to this equation exist for all initial conditions $u_0$ and all $t$. To this requires the Gronwall Inequality. This is basically the world's worst inequality, but it will be good enough for our use. There are numerous versions of this inequality. Here is one of them.\\

\begin{lemma}[Gronwall Inequality]
Let $u(t), g(t)$ be non-negative, real-valued functions defined on $t \in [t_0, t_1]$. Let $C \geq 0$ be a constant so that
\begin{align*}
u(t) \leq C + \int_{t_0}^t g(s) u(s) ds && t \in [t_0, t_1]
\end{align*}
Then
\begin{align*}
u(t) &\leq C \exp \left( \int_{t_0}^t g(s) ds \right) && t \in [t_0, t_1]
\end{align*}
If $C = 0$, then $u = 0$ for $t \in [t_0, t_1]$.
\begin{proof}
\begin{enumerate}
\item We first consider the case where $C > 0$. Let
\[
v(t) = C + \int_{t_0}^t g(s) u(s) ds
\]
Then $u(t) \leq v(t)$ on $[t_0, t_1]$ (by assumption), and $v(t) \leq C > 0$ on $[t_0, t_1]$ (since $g, u \geq 0$). Differentiating $v$ with respect to $t$, we have
\[
v'(t) = g(t) u(t) \leq g(t)v(t)
\] 
Since $v(t) > 0$ on $[t_0, t_1]$, we can divide by it to get
\begin{align*}
\dfrac{\dot{v}(t)}{v(t)} &\leq g(t) && t \in [t_0, t_1]
\end{align*}
Now, we integrate $g(s)$.
\begin{align*}
\int_{t_0}^t g(s) ds &\geq \int_{t_0}^t \dfrac{\dot{v}(s)}{v(s)} ds \\
&= \int_{t_0}^t \frac{d}{ds}\left( \log {v(s)} \right) ds \\
&= \log\dfrac{v(t)}{v(t_0)}\\
&= \log\dfrac{v(t)}{C}
\end{align*}
Exponentiate both sides and multiply by $C$ to get
\begin{align*}
u(t) &\leq C \exp \left( \int_{t_0}^t g(s) ds \right) && t \in [t_0, t_1]
\end{align*}
\item If $C = 0$, then for $\epsilon > 0$,
\begin{align*}
u(t) \leq \epsilon + \int_{t_0}^t g(s) u(s) ds && t \in [t_0, t_1]
\end{align*}
By step (i), for $t \in [t_0, t_1]$ 
\begin{align*}
u(t) &\leq \epsilon \exp \left( \int_{t_0}^t g(s) ds \right) \\&\leq \epsilon \exp \left( \int_{t_0}^{t_1} g(s) ds \right) \\
&\leq M \epsilon
\end{align*}
which is independent of $t$. Since $\epsilon$ is arbitrary, we conclude that $u = 0$ on $[t_0, t_1]$.
\end{enumerate}
\end{proof}
\end{lemma}

We can now show that $\dot{u} = A(t) u$ has a solution for all $t$ and all initial conditions $u_0$.\\

\begin{theorem}[Fundamental Matrix Solution]
Consider the system
\begin{align*}
\dot{u} &= A(t) u \\
u(s) &= u_0
\end{align*}
where $u \in \R^n$ and $A:\R \rightarrow \R^{n \times n} \in C^0$. Then there exists a unique $C^1$ function $\Phi: \R \times \R \rightarrow \R^n$, called the \emph{fundamental matrix solution}, such that for all $t, s, r \in \R$
\begin{enumerate}[(i)]
\item $\Phi(t, t) = I$ \\
\item $\Phi(t, r)\Phi(r, s) = \Phi(t,s)$
\item $u(t) = \Phi(t, s)u_0$ is the unique solution to the system with initial condition $u(s) = u_0$.
\end{enumerate}
\begin{proof}
\begin{enumerate}
\item Writing the ODE as $\dot{u} = f(u, t)$ with $f(u,t) = A(t) u$, $f(u, t)$ is locally Lipschitz in $u$ for each fixed $t$ (since it is linear in $u$ in that case.) Thus the initial value problem has a unique solution $u(t)$ for $t$ near $s$. 
\item We wish to show that $u(t)$ exists for all $t \in \R$. Suppose this solution exists on $[s, t_0)$, but does not exist beyond $t_0$. Then we know $u(t)$ must blow up as it approaches $t_0$. We will show this cannot happen.
\item We integrate the ODE to get
\begin{align*}
u(t) &= u_0 + \int_s^t A(\tau)u(\tau)d\tau && t \in [s, t_0)
\end{align*}
Taking absolute values and using the operator norm of $A(t)$, we get
\begin{align*}
|u(t)| &= |u_0| + \int_s^t ||A(\tau)||\:|u(\tau)|d\tau && t \in [s, t_0)
\end{align*}
\item This satisfies the hypotheses of Gronwall's Inequality. Thus, for $t \in [s, t_0)$
\begin{align*}
|u(t)| &\leq |u_0| \exp \left( \int_s^t ||A(\tau)|| d\tau \right) \\
&\leq |u_0| \exp \left( \int_s^{t_0} ||A(\tau)|| d\tau \right) \\.&\leq C |u_0|
\end{align*}
where the bound holds uniformly for $t \in [s, t_0)$ since it does not depend on $t$.
\item We conclude that $u(t)$ cannot blow up as $t \rightarrow t_0$, thus $u(t)$ must exist for all $t > s$. A similar argument shows that $u(t)$ must exist for all $t < s$.
\item Thus we can define $u(t) = \Phi(t, s)u_0$ to be the unique solution with initial condition $u(s) = u_0$. The properties of $\Phi(t,s)$ follow from the linearity of the system. $\Phi(t,s)$ is a matrix, and the $k$-th column of $\Phi(t,s)$ is the unique solution at time $t$ resulting from an initial condition $e_k$ at time $s$, where $e_k$ is the $k-$th standard unit vector in $\R^n$. 
\end{enumerate}
\end{proof}
\end{theorem}

We collect some properties of $\Phi(t,s)$ in the following lemma. We first give the following definition of a matrix solution.\\

\begin{definition}A \emph{matrix solution} of the ODE $\dot{u} = A(t)u, u \in \R^n$ is a $n \times n$ matrix $\Psi(t)$ whose columns are linearly independent solutions.
\end{definition}

Since the ODE is linear, matrix solutions are well defined, i.e. if the columns are linearly independent for some $t$ then they are linearly independent for all $t$.\\

\begin{lemma}
Let $\Phi(t, s)$ be the fundamental matrix solution for $\dot{u} = A(t) u$. Then
\begin{enumerate}[(i)]
\item $||\Phi(t,s)|| \leq \exp\left( \int_s^t ||A(\tau)|| d\tau \right)$
\item $\Phi(t,s)^{-1} = \Phi(s,t)$ \\
\item If we define $\Phi(t) = \Phi(t, 0)$, then $\Phi(t,s) = \Phi(t)\Phi(s)^{-1}$
\item Let $\Psi(t)$ be any $n\times n$ matrix solution of $\dot{u} = A(t) u$. Then 
	\begin{enumerate}[(a)]
	\item $\Psi(t) = \Phi(t) \Psi(0)$
	\item Liouville's formula states that
	\begin{equation}
	\det \Psi(t) = \det \Psi(s) \exp\left( \int_s^t \text{Tr }A(\tau) d \tau \right)
	\end{equation}
	\end{enumerate}
\end{enumerate}
\begin{proof}
(i) follows from the use of Gronwall's inequality in the previous theorem. (ii) follows since $\Phi(s,t)\Phi(t,s) = \Phi(t,s)\Phi(s,t) = I$ by uniqueness of solutions. (iii) follows from $\Phi(t,s) = \Phi(t,0)\Phi(0,s) = \Phi(t)\Phi(s)^{-1}$. (iv)(a) follows from the definition of the fundamental matrix solution $\Phi$.\\

For Liouville's formula (iv), we do the following.
\begin{enumerate}
\item Since $\Psi(t)$ is a matrix solution, its columns are linearly independent, so $\det \Psi(s) \neq 0$ for all $s$.
\item Take the matrix $\Psi(t)\Psi(s)^{-1}$ representing the flow of the system from $s$ to $t$, and write it in column form as
\[
\Psi(t)\Psi(s)^{-1} = \left[\Psi(t)\Psi(s)^{-1} e_1, \dots \Psi(t)\Psi(s)^{-1}e_n\right]
\]
\item Now take the derivative of $\det \Psi(t)\Psi(s)^{-1}$. Since the determinant is linear in each column (and row), the derivative of the determinant is the sum of the determinants where the derivative is applied to each column.
\begin{align*}
\frac{d}{dt} \det \Psi(t)\Psi(s)^{-1} &= \sum_j \det \left[\Psi(t)\Psi(s)^{-1} e_1, \dots \frac{d}{dt}\Psi(t)\Psi(s)^{-1}e_j, \dots, \Psi(t)\Psi(s)^{-1}e_n\right] \\
&= \sum_j \det \left[\Psi(t)\Psi(s)^{-1} e_1, \dots A(t) \Psi(t)\Psi(s)^{-1}e_j, \dots, \Psi(t)\Psi(s)^{-1}e_n\right]
\end{align*}
where we used the fact that $\Psi(t)\Psi(s)^{-1}$ satisfies the ODE.
\item Evaluate this at $t = s$ to get
\begin{align*}
\frac{d}{dt} \det \Psi(t)\Psi(s)^{-1}\Big|_{t = s} &= \sum_j \det \left[e_1, \dots, A(s) e_j, \dots, e_n\right] \\
&= \sum_j [A(s)]_{jj} \\
&= \text{Tr }A(s)
\end{align*}
\item Thus since $\Psi(s)$ is invertible, i.e. has nonzero determinant
\begin{align*}
\text{Tr }A(s) &= \frac{d}{dt} \det \Psi(t)\Psi(s)^{-1}\Big|_{t = s} \\
&= \frac{d}{dt} \det \Psi(t) \det(\Psi(s)^{-1}) \Big|_{t = s} \\ 
&= \frac{d}{dt} \det \Psi(t) \det(\Psi(s))^{-1} \Big|_{t = s} \\
\text{Tr }A(s) \det(\Psi(s)) &= \frac{d}{dt} \det \Psi(t)\Big|_{t = s}
\end{align*}
Since we are taking $s = t$, this becomes
\begin{align*}
\frac{d}{dt} \det(\Psi(t)) &= \text{Tr }A(t) \det(\Psi(t))
\end{align*}
\item Integrate this with initial condition at $s$ o get Liouville's formula.
\end{enumerate} 
\end{proof}
\end{lemma}

The standard application of Liouville's Formula is to find the final linearly independent solution of an $n-$th order linear system, when $n-1$ linearly independent solutions are already known. As an example, consider the system

\[
\dot{y} = 
\begin{pmatrix}1 & -1/t \\ 1 + t & -1 \end{pmatrix}
\] 
for $t \in (0, \infty)$ and $y \in \R^2$. Let $A(t)$ be the matrix. Suppose you know that $y(t) = (1, t)$ is a solution. Either you guessed it, or genie told you, or whatever. Let's use Liouville's Formula to find another linearly independent solution. Let $y = (y_1(t), y_2(t))$ be that solution. Thus 

\[
\Psi(t) = \begin{pmatrix}
y_1(t) & 1 \\
y_2(t) & t 
\end{pmatrix}
\]

is a matrix solution. Since $\text{Tr }A(t) = 0$, the integral in Liouville's formula vanishes, thus it reduces to $\det \Psi(t) = \det \Psi(s)$ for all $s,t > 0$. In other words, $\det Psi(t)$ is constant. Evaluating this, we have

\begin{align*}
c_1 &= \det \Psi(t) \\
&= t y_1(t) - y_2(t)
\end{align*}

which we can solve for $y_2$ to get
\[
y_2(t) = t y_1(t) - c_1
\]

We then plug this into the expression for $\dot{y_1}$ from the original ODE.

\begin{align*}
\dot{y_1}(t) &= y_1(t) - \frac{y_2(t)}{t} \\
&= y_1(t) - \frac{t y_1(t) - c_1}{t} \\
&= \frac{c_1}{t}
\end{align*}

which is independent of $y_1$ (!) Integrating this, we get

\[
y_1(t) = c_1 \log t + c_2
\]

which implies

\[
y_2(t) = c_1 t \log t + c_2 t - c_1
\]

We should be able to play with the constants to get another linearly independent solution. If we take $(c_1, c_2) = (0, 1)$, we recover the solution we already have. Taking $(c_1, c_2) = (1, 0)$ gives us another linearly independent solution $(\log t, t \log t - 1)$, which would have been much harder to guess. Thus a matrix solution to the system is

\[
\Psi(t) = \begin{pmatrix}
\log t & 1 \\
t \log t - 1 & t 
\end{pmatrix}
\]

Now that we have shown that the fundamental solution exists for linear ODEs of the form $\dot{u} = A(t) u$, we will show that a unique solution exists for the inhomogeneous ODE $\dot{t} = A(t)u + g(u, t)$. Even better, we have a formula for the solution!\\

\begin{theorem}[Variation of Constants Formulas]\label{VOC}
\begin{enumerate}[(i)]
\item Consider the ODE
\begin{align*}
\dot{u} &= A(t)u + g(t) \\
u(s) &= u_0
\end{align*}
with $A(t)$ and $h(t)$ continuous. Then the unique solution, which is valid for all $t \in \R$ is given by
\begin{equation}\label{VOC1}
u(t) = \Phi(t,s)u_0 + \int_s^t \Phi(t, \tau) g(\tau) d \tau
\end{equation}
\item Consider instead the ODE
\begin{align*}
\dot{u} &= A(t)u + g(u, t) \\
u(s) &= u_0
\end{align*}
where this time $g$ depends on $u$ as well. Suppose $g$ is locally Lipschitz, and let $u(t)$ be the unique solution on an interval $I$ containing $s$. Then for all $t \in I$, $u(t)$ can be written as
\begin{equation}\label{VOC2}
u(t) = \Phi(t,s)u_0 + \int_s^t \Phi(t, \tau) g(u(\tau), \tau) d \tau
\end{equation}
Note that this is not really a formula for the solution $u(t)$ since $u$ appears on both sides.
\end{enumerate}
\begin{proof}
For (i), you can be clever and derive the formula, but honestly it is easiest just to plug it in and verify that it satisfies the ODE. Since $||\Phi(t,\tau)|| \leq \exp\left( \int_s^t ||A(z)|| dz \right)$ by Gronwall's Inequality and $h(\tau)$ is bounded on $[s, t]$ by continuity, the integral always has finite magnitude, thus the solution does not blow up and is defined for all $t$.\\

Similarly, you can verify (ii) by plugging in the formula.
\end{proof}
\end{theorem}

% Finally, we look at the variational equation, which shows up all the time in applications.

% Consider the ODE
% \[
% \dot{u} = f(u)
% \]
% with $u \in \R^n$ and $f \in C^1$. Let $\Phi_t(u)$ be the flow of the system. Since $f$ does not depend on $t$, this flow is independent of what time we use for our initial condition, so we can always take it at $t = 0$. Thus, for all $t \in \R$, $u \in \R^n$ we have

% \begin{align*}
% \frac{d}{dt}\Phi_t(u) &= f(\Phi_t(u)) \\
% \Phi_0(u) &= u
% \end{align*}

% Now we differentiate this in $u$ at $u = u_0$.

% \begin{align*}
% \frac{d}{dt} D\Phi_t(u_0) &= Df(\Phi_t(u_0)) D\Phi_t(u_0)
% \end{align*}

\subsection{Floquet Theory}

Floquet theory is a tool used to analyze linear ODEs $\dot{u} = A(t) u$ the case where the matrix $A(t)$ is periodic. For motivation, consider the system

\begin{align*}
\dot{u} &= f(u) \\
u(0) &= u_0
\end{align*}

where $f$ is smooth, and suppose we have a solution $q(t)$ which is a periodic orbit, i.e. $q(t + T) = q(t)$ for all $t$. Linearizing about this solution, we obtain the linear ODE

\[
\dot{v}(t) = DF(q(t)) v(t)
\]

Since $f$ does not depend on $t$, the dependence of $DF(q(t))$ on $t$ will be via $q(t)$ and its derivatives, all of which are periodic with period $T$. Thus $DF(q(t))$ will be periodic with period $T$. \\

With that example in mind, we will consider the linear system

\begin{align*}
\dot{u} &= A(t) u
\end{align*}

where $u \in \R^n$, $A(t)$ is a continuous $n \times n$ matrix which is periodic with period $T$. The Floquet theorem lets us decompose the fundamental matrix solution $\Phi(t)$ into the product of a periodic matrix and a matrix exponential.\\

\begin{theorem}[Floquet]
Let $\Phi(t)$ be the fundamental matrix solution to $\dot{u} = A(t) u$, where $A(t)$ is continuous and $T-$periodic. Then
\begin{enumerate}[(i)]
\item There exists a matrix $B$ (which may be complex) and a $T-$periodic matrix $P(t)$ (which may be complex valued) with $P(0) = I$ such that 
\[
\Phi(t) = R(t)e^{Bt}
\]
\item There exists a real matrix $R$ and a real-valued $2T-$periodic matrix $Q(t)$ with $Q(0) = I$ such that
\[
\Phi(t) = Q(t)e^{Rt}
\]
\end{enumerate}
\begin{proof}
We first prove (i)
\begin{enumerate}
\item First, we show that $\Phi(t+T)$ is a matrix solution for the system.
\begin{align*}
\frac{d}{dt}\Phi(t+T) &= A(t+T) \\
&= A(t)\Phi(t+T)
\end{align*}
since $A(t)$ is $T$-periodic. Since $\Phi(t)$ is a flow, we also have $\Phi(t+T) = \Phi(t)\Phi(T)$.
\item Let $B$ be any complex-valued matrix, and define
\[
P(t) = \Phi(t) e^{-Bt}
\]
This definition implies that $P(0) = I$. Then we have
\begin{align*}
P(t+T) &= \Phi(t+T) e^{-B(t+T)} \\
&= \Phi(t)\Phi(T) e^{-B T} e^{-B t}
\end{align*}
If we can find a matrix $B$ so that $\Phi(T) e^{-B T} = I$, i.e. $e^{BT} = \Phi{T}$, then 
\begin{align*}
P(t+T) &= \Phi(t) e^{-B t} = P(t)
\end{align*}
Thus $P$ is $T-$periodic and $\Phi(t) = P(t) e^{Bt}$.
\item We have reduced the problem to finding such a $B$. First, we note that $\Phi(T)$ is invertible, since it is a fundamental matrix solution. Next, we transform $\Phi(T)$ into Jordan canonical form $J$ via an invertible matrix $S$, so we reduce our problem to 
\begin{equation}
J = S^{-1}\Phi(T)S = S^{-1}e^{BT}S 
= e^{S^{-1} B S T} = e^{C}
\end{equation}
Thus, we only have to solve $e^{CT} = J$ for a complex matrix $C$, where $J$ has no eigenvalues of 0 since $\Phi(T)$ is invertible. 
\item Since $J$ is a block diagonal matrix composed of Jordan blocks, since the matrix exponential acts on each Jordan block independently, it suffices to consider a single $k \times k$ Jordan block $J$. This Jordan block has the form $J = \lambda I + N$, where $\lambda \neq 0$ and $N$ is the standard nilpotent matrix consisting of ones on the first upper diagonal. Since $\lambda \neq 0$, we write this as
\[
J = \lambda\left(I + \frac{1}{\lambda}N\right)
\]
Thus we only need to solve
\[
e^{C} = \lambda\left(I + \frac{1}{\lambda}N\right)
\]
for a $k \times k$ matrix $C$.
\item Next, we get rid of the $\lambda$ out front. Let $\tilde{C} = (\log \lambda)I + \tilde{C}$. Substituting this in, and noting that $\log \lambda)I$ and $\tilde{C}$ commute, we have
\begin{align*}
e^{(\log \lambda)I + \tilde{C}} &= \lambda\left(I + \frac{1}{\lambda}N\right) \\
e^{(\log \lambda)I} e^{\tilde{C}} &= \lambda\left(I + \frac{1}{\lambda}N\right) \\
\lambda I e^{\tilde C} &= \lambda\left(I + \frac{1}{\lambda}N\right) \\
e^{\tilde C} &= \left(I + \frac{1}{\lambda}N\right) \\
\end{align*}
Thus, we only need to solve 
\[
e^{\tilde C} = \left(I + \frac{1}{\lambda}N\right)
\]
for a $k \times k$ matrix $\tilde{C}$. We solve this by taking the ``matrix logarithm''. The formal Taylor series for $\log(I + M)$ is
\[
\log(I + M) = \sum_{n=1}^\infty (-1)^{n+1} \frac{M^n}{n}
\]
where this is only formal since there is no guarantee this will converge unless $||M|| < 1$. If this series does converge, however, it follows that
\[
e^{\log(I+M)} = I + M
\]
There is one more case where the series is guaranteed to converge, which happens in our case. The formal Taylor series for $\log(I + \frac{1}{\lambda}N)$ converges since $N$ is nilpotent ($N^k = 0$), thus the sum is actually a finite sum! Thus we conclude that 
\[
\tilde{C} = \log\left( I + \frac{1}{\lambda}N \right) = \sum_{n=1}^{k-1} \frac{(-1)^{n+1}}{n} \left( \frac{1}{\lambda}N \right)^n
\]
\end{enumerate}

For part (ii), we note that

\[
\Phi(t + 2T) = \Phi(t)\Phi(T)^2
\]

By a similar argument we can show that there exists a real matrix $R$ such that $e^{2RT} = \Phi(T)^2$. Letting $Q(t) = \Phi(t)e^{-Rt}$, it follows thtat

\begin{align*}
Q(t+2T) &= \Phi(t+2T)e^{-2RT}e^{-Rt} \\
&= \Phi(t)\Phi(T)^2 e^{-2RT} e^{-Rt} \\
&= \Phi(t) e^{-Rt} \\
&= Q(t)
\end{align*}

thus $Q(t)$ is $2T-$periodic, everything is real, and we can write

\[
\Phi(t) = Q(t) e^{Rt}
\]

\end{proof}
\end{theorem}

The representation $\Phi(t) = P(t)e^{Bt}$ from the Floquet Theorem is called the \emph{Floquet normal form} for the fundamental matrix solution $\Phi(t)$. We use this normal form to examine the stability of the equilibrium at 0 in the $T-$periodic linear system $\dot{u} = A(t) u$. If this system comes from the linearization about a periodic orbit, this then tells us about the linear stability of that periodic orbit.\\

Take an initial condition $u_0$ at $t = 0$, and let $u(t)$ be the unique solution with $u(0) = u_0$. Then every time we move forward by $T$, we apply the map $\Phi(T)$, i.e.

\begin{align*}
u(T) &= \Phi(T) u_0 \\
u(2T) &= \Phi(T)^2 u_0 \\
&\vdots
\end{align*}

The operator $\Phi(T)$ is known as the \emph{monodromy matrix} or \emph{monodromy operator}. By the Floquet theorem, this operator can be written as

\[
M = \Phi(T) = P(T)e^{BT} = P(0) e^{BT} = e^{BT}
\]

since $P$ is $T-$periodic, with $P(0) = I$. Thus we can study stability of the equilibrium at 0 by looking at the discrete dynamical system

\[
v_{n+1} = M v_n
\]

where $M = e^{BT}$ is a constant matrix. From the theory of discrete dynamical systems, the stability of 0 depends on the eigenvalues of $M$, which are called the \emph{Floquet multipliers}. If $|\rho| < 1$ for all Floquet multipliers $\rho$ of $M$, then 0 is stable (in the discrete system), thus 0 is stable (in the original system), and so $||\Phi(t)|| \rightarrow 0$ as $t \rightarrow \infty$.\\

The eigenvalues of $B$ are known as the \emph{Floquet exponents}. The stability criterion can also be stated in terms of the Floquet exponents: If $\text{Re }\lambda < 0$ for all Floquet exponents $\lambda$, then $||\Phi(t)|| \rightarrow 0$ as $t \rightarrow \infty$.\\

Finally, we remark that since the Floquet multipliers are eigenvalues of a matrix $M$, they are unique. The Floquet exponents are not unique, since we can derive another Floquet normal form for the system.

\begin{align*}
\Phi(t) &= R(t) e^{Bt}  \\
&= R(t) e^{-2\pi i t/T}e^{2\pi i t/T}e^{Bt} \\
&= [ R(t) e^{-2\pi i t/T} ] e^{[B + (2 \pi i/T) I]t}
\end{align*}

$\tilde{R}(t) = R(t) e^{-2\pi i t/T}$ is still $T-$periodic, thus this is a valid Floquet normal form. We have a new matrix $\tilde{B} = B + (2 \pi i/T) I$, whose eigenvalues are shifted by $2 \pi i/T$. Thus Floquet exponents are only unique modulo $2 \pi i/T$.

\subsection{Stable Manifold Theorem}

Consider the autonomous ODE
\begin{equation}
\dot{u} = f(u)
\end{equation}
where $u \in \R^n$, $f: \R^n \rightarrow R^n$ is smooth, and $f(0) = 0$. Expanding $f$ in a Taylor series about $u = 0$, this becomes

\begin{align*}
\dot{u} &= f(0) + Df(0)u + \mathcal{O}(|u|^2) \\
&= Au + g(u)
\end{align*}
where $A = Df(0)$ is a constant-coefficient matrix. From the Taylor theorem, $g(u) = \mathcal{O}(|u|^2)$, thus $g(0) = 0$ and $Dg(0) = 0$. Essentially, we ``extracted'' the linear terms into $A$, so $g$ is quadratic or higher order. We know exactly how the constant coefficient linear system $\dot{u} = Au$ behaves. We would like to relate the behavior of the linearized system to that of the original, nonlinear system.\\

The Hartman-Grobman theorem states that if 0 is a hyperbolic equilibrium of the linearized system, then the qualitative behavior of the nonlinear system near 0 is the same. We would like to be able to say more. In particular, we would like to show that the stable and unstable manifolds of the linear system perturb in some meaningful way to give us corresponding stable and unstable manifolds of the nonlinear system. To that end, we state and prove the Stable Manifold Theorem.\\

\begin{theorem}[Stable Manifold Theorem]
Consider the autonomous ODE
\begin{equation}
\dot{u} = Au + g(u)
\end{equation}
where $A$ is hyperbolic (i.e. has no eigenvalues with real part 0), $g \in C^k$ for $k \geq 1$, $g(0) = 0$, and $Dg(0) = 0$. Let $E^s$ and $E^u$ be the stable and unstable eigenspaces of $A$. Then there exists $r > 0$ and a manifold $W^s_{loc}(0)$, the local stable manifold at 0, which is given by

\[
W^s_{loc}(0) = \{ (a, h(a)) : a \in E^s, |a| \leq r \}
\]

where $h: E^s \rightarrow E^u$ is $C^k$ with $h(0) = 0$ and $Dh(0) = 0$, i.e. $W^s_{loc}(0)$ is tangent to $E^s$ at $u = 0$.\\

If $\Phi(t)$ is the flow of the system, $W^s_{loc}(0)$ is forward invariant under the flow $\Phi(t)$, i.e. if $u_0 \in W^s_{loc}(0)$, then $\Phi(t)u_0 \in W^s_{loc}(0)$ for all $t \geq 0$. \\

Finally, initial conditions $u_0$ in $W^s_{loc}(0)$ decay exponentially to 0 under the flow, i.e. for all $u_0 \in W^s_{loc}(0)$, $\Phi(t)u_0 \rightarrow 0$ exponentially as $t \rightarrow 0$.

\begin{proof}
The proof is lengthy, but the basic idea is as follows. Essentially, we use variation of constants to write the ODE in integrated form, them project onto $E^s$ and $E^u$ to get two equations. We solve the $E^u$ equation for the initial condition in the unstable subspace, and then plug this into the $E^s$ equation. This gives us a contraction map between Banach spaces, to which we apply the uniform contraction mapping principle. 

\begin{enumerate}
\item Since $A$ is hyperbolic, we can find $\alpha > 0$ such that $\text{Re }(\lambda)$ lies outside $(-\alpha, \alpha)$ for all eigenvalues $\lambda$ of $A$.

\item We use hyperbolicity to get estimates on the matrix exponential $e^{At}$. Let $P^s$ and $P^u$ be the spectral projections on $E^s$ and $E^u$. Since they are spectral projections, they commute with $A$, and thus commute with $e^{At}$. Then things in the stable subspace decay exponentially in forward time, and things in the unstable subspace decay exponentially in backward time. In other words, there exists $M \geq 1$ such that
\begin{align*}
||e^{At} P^s || &\leq M e^{-\alpha t} && t \geq 0 \\
||e^{At} P^u || &\leq M e^{\alpha t} && t \leq 0
\end{align*}

\item Next, we look at the function $g$. Since $g$ is at least $C^1$, it is locally Lipshitz. Thus, for every $\rho > 0$, $g$ is Lipschitz on $B(0, \rho)$ with Lipschitz constant $L(\rho) = \max\{||Dg(u)||: |u| \leq \rho\}$. Furthermore, since $Dg(0) = 0$ and $Dg$ is continuous, $L(\rho) \rightarrow 0$ as $\rho \rightarrow 0$. Since $g(0) = 0$, this implies that $|g(u)| \leq L(\rho)|u|$ whenever $|u| \leq \rho$.

\item Suppose $u(t)$ satisfies $\dot{u} = Au + g(u)$ with $u(0) = u_0$. Using the Variation of Constants Formula, we write $u(t)$ in integrated form as
\[
u(t) = e^{At} u_0 + \int_0^t e^{A(t-s)}g(u(s)) ds
\]

\item Apply the projections $P^s$ and $P^u$ to the integrated form. Recalling that the projections commute with $e^{At}$, these become

\begin{align*}
P^u u(t) &= e^{At} P^u u_0 + \int_0^t e^{A(t-s)} P^u g(u(s)) ds \\
P^s u(t) &= e^{At} P^s u_0 + \int_0^t e^{A(t-s)} P^s g(u(s)) ds
\end{align*}
We will call these the unstable and stable equations. 

\item What we want to do now is solve the ``unstable'' equation for $P^s(u_0)$ for a suitable set of functions $u(t)$. Suppose $u(t)$ is bounded for $t \geq 0$, i.e. $|u(t)| \leq \rho$ for $t \geq 0$. (The actual bound does not matter for now, only that there is one.)\\

Multiplying the unstable equation by $e^{-At}$ on the right, we get

\begin{align*}
e^{-At} P^u u(t) &= P^u u_0 + \int_0^t e^{-As} P^u g(u(s)) ds
\end{align*}

We want to solve for $P^u u_0$ independently from $t$, so one strategy is to send $t \rightarrow \infty$. If we do that, the term on the LHS vanishes, since

\begin{align*}
| e^{-At} P^u u(t) | &\leq M e^{-\alpha t} |u(t)| \\
&\leq M \rho e^{-\alpha t}
\end{align*}

The indefinite integral converges, since

\begin{align*}
\left| \int_0^\infty e^{-As} P^u g(u(s)) ds \right| 
&\leq \int_0^\infty || e^{-As} P^u || L(\rho) |u(s)| ds \\
&\leq M L(\rho) \rho \int_0^\infty e^{-\alpha s} ds < \infty
\end{align*}

Thus, as long as $u(t)$ is bounded for $t \geq 0$, we can solve for $P^u u_0$ to get

\begin{align*}
P^u u_0 &= -\int_0^\infty e^{-As} P^u g(u(s)) ds
\end{align*}

\item Now, we add together the stable and unstable equations. Since $P^s + P^u = I$, the LHS is just $u(t)$. 

\begin{align*}
u(t) &= e^{At} P^s u_0 + e^{At} P^u u_0 + \int_0^t e^{A(t-s)} P^u g(u(s)) ds + \int_0^t e^{A(t-s)} P^s g(u(s)) ds
\end{align*}

Substituting in our expression for $P^u u_0$, we get

\begin{align*}
u(t) &= e^{At} P^s u_0 - e^{At} \int_0^\infty e^{-As} P^u g(u(s)) ds + \int_0^t e^{A(t-s)} P^u g(u(s)) ds + \int_0^t e^{A(t-s)} P^s g(u(s)) ds \\
&= e^{At} P^s u_0 + \int_\infty^0 e^{A(t-s)} P^u g(u(s)) ds + \int_0^t e^{A(t-s)} P^u g(u(s)) ds + \int_0^t e^{A(t-s)} P^s g(u(s)) ds \\
&= e^{At} P^s u_0 + \int_\infty^t e^{A(t-s)} P^u g(u(s)) ds + \int_0^t e^{A(t-s)} P^s g(u(s)) ds \\
\end{align*}

As long as $u(t)$ is bounded for $t \geq 0$, this formula is valid.

\item Let $a = P^s u_0 \in E^s$. Then define the map $F(u, a)$ by the RHS of the RHS of the expression above.

\[
[F(u, a)](t) = e^{At} P^s a + \int_\infty^t e^{A(t-s)} P^u g(u(s)) ds + \int_0^t e^{A(t-s)} P^s g(u(s)) ds
\]

where we added an extra $P^s$ in front of $a$ since $P^s$ is the identity on $E^s$.\\

Note that we have not yet specified what spaces this maps from and to. Since we want to use the Uniform Contracting Mapping Principle, we want to show that $F: D \times B \rightarrow D$, where $D$ is a closed subset of a Banach space $X$, and $B$ is a subset of $\R^n$. There are a few choices for Banach spaces $X$. We will use an exponentially weighted space, since it will make our lives easier in the end. Choose any $\beta$ with $0 < \beta < \alpha$, and define the norm

\[
||f||_\beta = \sup_{t \geq 0}|e^{\beta t}f(t)|
\]

We note that $||f||_\beta \leq \rho$ implies $\sup_{t \geq 0} f(t)$. Then the space

\[
X_\beta = \{ f \in C^0([0, \infty), \R^n) : ||f||_\beta < \infty \}
\]

is a Banach space. Note that every function $u(t)$in this space is bounded, so our formula above is valid for it.\\

Let

\[
D = \{ u \in X_\beta : ||u||_\beta \leq \rho \}
\]

where $\rho > 0$ will be chosen later. For the parameter space, let

\[
B = \{a \in E^s : |a| \leq r \}
\]

where $r > 0$ will be chosen later.

\item We will now verify the hypotheses of the contraction mapping principle. As we do this, we will choose the values of the two radii $\rho$ and $r$. (It doesn't matter what they are, as long as they exist.) First, we show that $F: D \times B \rightarrow D$. Let $(u, a) \in D \times B$. 

Then

\begin{align*}
|e^{\beta t} [F(u, a)](t)| &\leq e^{\beta t} || e^{At} P^s || \: |a| + \int_t^\infty e^{-\beta t} || e^{A(t-s)} P^u||\:|g(u(s))| ds + \int_0^t || e^{A(t-s)} P^s|| \: |g(u(s))| ds \\
&\leq e^{\beta t} M e^{-\alpha t} r + M \int_t^\infty e^{\beta t} e^{\alpha(t-s)}L(\rho)|u(s)| ds + M \int_0^t e^{\beta t} e^{-\alpha(t-s)} L(\rho)|u(s)| ds \\ 
&= M e^{-(\alpha - \beta) t} r + L(\rho) M \int_t^\infty e^{\beta (t-s)} e^{\alpha(t-s)}|e^{\beta s} u(s)| ds +  L(\rho) M \int_0^t e^{\beta (t-s)} e^{-\alpha(t-s)}|e^{\beta s} u(s)| ds \\
&= M r + L(\rho)||u||_\beta M \int_t^\infty e^{-(\alpha + \beta)(s-t)} ds +  L(\rho) ||u||_\beta M \int_0^t e^{-(\alpha - \beta)(t-s)} ds \\
&= M r + L(\rho) \rho M \left( \frac{1}{\alpha + \beta} + \frac{1}{\alpha - \beta} \right) \\
\end{align*}

Since $L(\rho) \rightarrow 0$ as $\rho \rightarrow 0$, first choose $\rho$ sufficiently small so that 

\[
L(\rho) \leq \frac{1}{2M}\left( \frac{1}{\alpha + \beta} + \frac{1}{\alpha - \beta} \right)^{-1}
\]

That gives us 

\[
|e^{\beta t} [F(u, a)](t)| \leq M r + \frac{\rho}{2}
\]

Finally choose $r \leq \frac{\rho}{2M} $ to get

\[
|e^{\beta t} [F(u, a)](t)| \leq \rho 
\]

Thus we have shown that $F: D \times B \rightarrow B$.

\item We next show that $F$ is a uniform contraction. Similar to what we did in the previous step, we evaluate

\begin{align*}
|e^{\beta t} &([F(u, a)](t) - [F(v,a)](t))| \\
&\leq M \int_t^\infty e^{\beta t} e^{\alpha(t-s)} |g(u(s)) - g(v(s))| ds + M \int_0^t e^{\beta t} e^{-\alpha(t-s)}|g(u(s)) - g(v(s))| ds \\
&\leq M \int_t^\infty e^{\beta t} e^{\alpha(t-s)} L(\rho) |u(s) - v(s)| ds + M \int_0^t e^{\beta t} e^{-\alpha(t-s)}L(\rho) |u(s) - v(s)| ds \\
&= M \int_t^\infty e^{\beta(t-s)} e^{\alpha(t-s)} L(\rho) |e^{\beta s}(u(s) - v(s))| ds + M \int_0^t e^{\beta(t-s)} e^{-\alpha(t-s)}L(\rho)|e^{\beta s}(u(s) - v(s))| ds \\ 
&\leq M L(\rho) ||u - v||_\beta \left( \int_t^\infty e^{-(\alpha + \beta)(s-t)} ds +   \int_0^t e^{-(\alpha - \beta)(t-s)} ds \right) \\
&= M L(\rho) ||u - v||_\beta \left( \frac{1}{\alpha + \beta} + \frac{1}{\alpha - \beta} \right) \\
&\leq \frac{1}{2}||u - v||_\beta
\end{align*}

Taking the supremum over all $t \geq 0$, we conclude that for all $a \in B$, 

\[
||F(u, a) - F(v,a)||_\beta \leq \frac{1}{2}||u - v||_\beta
\]

Thus $F$ is a contraction.

\item By the Uniform Contraction Mapping Principle, there is a unique map $G: B \rightarrow D$ which maps any $a \in B$ to the unique fixed point of $F(\cdot, a)$, i.e. $F(G(a), a) = a$ for all $a \in B$.\\

$F$ is linear in $a$, thus is $C^\infty$ in $a$. From the chain rule for Frechet derivatives, if $g$ is $C^k$, then $F$ is $C^k$ in $u$. Since $F: D \times B \rightarrow D$ is $C^K$, the map $G: B \rightarrow D$ is $C^k$ as well.

\item We can now use $G$ to define the local stable manifold. We remind ourselves that $G(a)$ is a function from $[0, \infty)$ to $\R^n$ which lies in $D$. Thus we define the local stable manifold to be the initial value of all of these functions for all $a \in B$.

\[
W^s_{loc}(0) = \{ u_0 = G(a)(0) : a \in B \}
\] 

Since $G(a)$ is a fixed point of $F(G(a), a)$,

\[
G(a)(t) = e^{At} P^s a + \int_\infty^t e^{A(t-s)} P^u g(G(a)(s)) ds + \int_0^t e^{A(t-s)} P^s g(G(a)(s)) ds
\]

thus since $P^s a = a$,

\[
G(a)(0) = a + \int_\infty^0 e^{-As} P^u g(G(a)(s)) ds
\]

The integral term above is in $E^u$. Thus we define $h: B \rightarrow E^u$ by

\[
h(a) = \int_\infty^0 e^{-As} P^u g(G(a)(s)) ds
\]

Thus the local stable manifold is the graph of $h(a)$, where $a \in B$, i.e.

\[
W^s_{loc}(0) = \{ (a, h(a)) : a \in B \}
\]

Since $g(0) = 0$, $F(0, 0) = 0$ (the second 0 is the 0 function), thus since $G$ maps to the unique fixed point of $F$, $G(0) = 0$. It then follows from $g(0) = Dg(0) = 0$ that $h(0) = Dh(0) = 0$. Thus $W^s_{loc}$ is tangent to $E^s$ at 0.

\item Next, we show that for any initial condition $u_0$ in $W^s_{loc}(0)$, the solution $u(t)$ to the system starting at $u_0$ decays exponentially to 0. We basically get this for free, since we used an exponentially weighted space. Take any point $(a, h(a) = (a, G(a)(0))$ as an initial condition. Recall that

\[
G(a)(t) = e^{At} P^s a + \int_\infty^t e^{A(t-s)} P^u g(G(a)(s)) ds + \int_0^t e^{A(t-s)} P^s g(G(a)(s)) ds
\]

Using the Leibniz rule for differentiating integrals, we differentiate this with respect to $t$ to get 

\begin{align*}
\frac{d}{dt}G(a)(t) &= A e^{At} P^s a + P^u g(G(a)(t)) + \int_\infty^t A e^{A(t-s)} P^u g(G(a)(s))) ds \\
&+ P^s g(G(a)(t))
+ \int_0^t A e^{A(t-s)} P^s g(G(a)(s)) ds \\
&= A\left(e^{At} P^s a + \int_\infty^t e^{A(t-s)} P^u g(G(a)(s)) ds + \int_0^t e^{A(t-s)} P^s g(G(a)(s)) ds \right) \\
&+ (P^u + P^s)g(G(a)(t)) \\
&= A G(a)(t) + g(G(a)(t))
\end{align*}

Thus $u(t) = G(a)(t)$ is the unique solution to the system with initial condition $(a, h(a)) \in W^s_{loc}(0)$. Since $u(t) \in D$, $|e^{\beta t}u(t)|\leq \rho$, which implies for $t \geq 0$
\[
|u(t)|\leq \rho e^{-\beta t}
\]
Thus $u(t)$ decays exponentially to 0.

\item Finally, we show that $W^s_{loc}(0)$ is invariant under the flow of the system. If we start at an initial condition $(a, h(a)) = (a, G(a)(0))$ then at time $t$ we have 

\[
G(a)(t) = e^{At} P^s a + \int_\infty^t e^{A(t-s)} P^u g(G(a)(s)) ds + \int_0^t e^{A(t-s)} P^s g(G(a)(s)) ds
\]

We need to show that $G(a)(t) \in W^s_{loc}$, i.e. 
$G(a)(t) = (a, h(a))$ for some $a$. Of course, we would like that $a$ to be $P^s G(a)(t)$. Since the equation for $G(a)(t)$ is naturally separated into stable and unstable components, we can write

\[
G(a)(t) = P^s G(a)(t) + \int_\infty^t e^{A(t-s)} P^u g(G(a)(s)) ds
\]

All that remains is to show that the second term on the RHS is $h( P^s G(a)(t) )$. Since $G(a)(t)$ is a solution to the ODE, by the properties of the flow of a dynamical system we have

\[
G( P^s G(a)(t) )(s) = G(a)(t+s)
\] 

Thus we have

\begin{align*}
h( P^s G(a)(t) ) &= \int_\infty^0 e^{-As} P^u g[G(P^s G(a)(t))(s)] ds \\
&= \int_\infty^0 e^{-As} P^u g(G(a)(t+s)) ds \\
&= \int_\infty^t e^{A(t-s)} P^u g(G(a)(s)) ds
\end{align*}

where we changed variables $t + s \rightarrow s$ in the last line. This is the RHS above, so we are all set! 

\end{enumerate}
\end{proof} 
\end{theorem}

Here are some important consequences of the stable manifold theorem.
\begin{enumerate}

\item We can define the stable manifold at 0 by running the points in the local stable manifold backwards in time under the flow. The points in the stable manifold eventually wind up in the local stable manifold as time flows forwards.

\begin{align*}
W^s(0) &= \{ u_0 : \Phi_t(u_0) \in W^s_{loc}(0) \text{ for some }t\geq 0 \}\\
&= \{ u_0 : \Phi_t(u_0) \rightarrow 0 \text{ as } t \rightarrow \infty \} 
\end{align*}

\item We can similarly define the local unstable manifold $W^u_{loc}(0)$ by reversing time.

\item If $u_0 \in W^s_{loc}(0)$, then the flow starting there remains in $W^s_{loc}(0)$ for $t \geq 0$ since $W^s_{loc}(0)$ is invariant. In other words, $u(t) = \Phi_t(u_0) \in W^s_{loc}(0)$ for $t \geq 0$. Using the Stable Manifold Theorem, we can write the solution $u(t)$ as

\[
u(t) = v(t) + h(v(t))
\]

where $v(t) \in E^s$ and $h(v(t)) \in E^u$. Since $u(t)$ solves the original ODE, we have

\begin{align*}
\dot{u}(t) = \dot{v}(t) + \frac{d}{dt}h(v(t)) 
&= A(v(t) + h(v(t)) ) + g(v(t) + h(v(t)) \\
&= A(v(t)) + A(h(v(t))) + g(v(t) + h(v(t))
\end{align*}

Taking the projection on $E^s$, we conclude that $v(t)$ satisfies the ODE

\begin{align*}
\dot{v} &= Av + P^s g(v + h(v)) \\
v(0) = P^s u_0
\end{align*}

Thus we have an ODE for the flow on the local stable manifold.

\end{enumerate}

Finally, there is a parameter-dependent version of the stable manifold theorej.

\begin{theorem}[Parameter Dependent Stable Manifold Theorem]
Consider the autonomous ODE
\begin{equation}
\dot{u} = f(u, \mu)
\end{equation}
where $u \in \R^n$, $\mu \in \R^p$. Let $f \in C^k, k \geq 1$ with $f(0, 0) = 0$ and $A = f_u(0, 0)$ hyperbolic with stable and unstable eigenspaces $E^s$ and $E^u$. Let $\Phi_t(u_0; \mu)$ be the flow of the dynamical system. \\

Then there are constants $r > 0$ and $\mu_0 > 0$ such that

\begin{enumerate}
\item For all $|\mu| \leq \mu_0$, the ODE has a unique equilibrium $u^*(\mu) \in B_r(0)$, with $u^*(0) = 0$

\item There exists a $C^k$ map 

\[
h : \{ a \in E^s: |a| \leq r \} \times \{ \mu \in \R^p : |\mu| \leq \mu_0 \} \rightarrow E^u
\]

with $h(0, 0) = 0$ and $h_u(0, 0) = 0$ such that the local stable manifold is given by

\[
W^s_{loc}(u^*(\mu), \mu) = \{ (a, h(a, \mu)) : a \in E^s, |a| \leq r, |\mu| \leq \mu_0 \}
\]

$W^s_{loc}(u^*(\mu), \mu)$ is forward invariant under the flow $\Phi_t(\cdot, \mu)$. Initial conditions in $W^s_{loc}(u^*(\mu), \mu)$ decay exponentially to $u^*(\mu)$.\\

\item $W^s_{loc}(u^*(\mu), \mu)$ is tangent to the stable eigenspace of $f_u(u^*(\mu),\mu)$ at $u^*(\mu)$.
\end{enumerate}
\end{theorem}





\end{document}