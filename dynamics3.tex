\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont}
\usepackage[shortlabels]{enumitem} 
\newcounter{question}
\setcounter{question}{0}
\usepackage{parskip}
\setlength{\parindent}{0pt}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{ esint }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{range}

\title{Dynamics Notes 3}
\author{Ross Parker}

\begin{document}

\section{Linear Systems}

Consider the ODE

\[
\dot{u} = f(u)
\]

with $u \in \R^n$ and $f \in C^2$, and suppose $u^*(t)$ is a solution. Let $u(t) = u^*(t) + v(t)$ be a perturbation of $u^*(t)$, where (hopefully) $v(t)$ is small. Then $u(t)$ satsfies the ODE if and only if the perturbation $v(t)$ satisfies

\begin{align*}
\dot{v}(t) &= \dot{u}(t) - \dot{u}^*(t) \\
&= f(u(t)) - f(u^*(t)) \\
&= f(u^*(t) + v(t)) - f(u^*(t)) \\
&= f(u^*(t)) + DF(u^*(t))v(t) + \mathcal{O}|v(t)|^2 - f(u^*(t)) \\
&= DF(u^*(t))v(t) + \mathcal{O}|v(t)|^2 
\end{align*}

where we used the trusty Taylor theorem on $f$. Thus, assuming $v(t)$ is small, this system is approximately the linear system

\begin{align*}
\dot{v}(t) &= DF(u^*(t))v(t) 
\end{align*}

which is the linearization of the original system about the solution $u^*(t)$. Thus is will be useful to consider systems of the form

\[
\dot{u} = A(t) u
\]

where $u \in \R^n$ and $A:\R \rightarrow \R^{n \times n}$ is continuous. What we would like to do is show that solutions to this equation exist for all initial conditions $u_0$ and all $t$. To this requires the Gronwall Inequality. This is basically the world's worst inequality, but it will be good enough for our use. There are numerous versions of this inequality. Here is one of them.

\begin{lemma}[Gronwall Inequality]
Let $u(t), g(t)$ be non-negative, real-valued functions defined on $t \in [t_0, t_1]$. Let $C \geq 0$ be a constant so that
\begin{align*}
u(t) \leq C + \int_{t_0}^t g(s) u(s) ds && t \in [t_0, t_1]
\end{align*}
Then
\begin{align*}
u(t) &\leq C \exp \left( \int_{t_0}^t g(s) ds \right) && t \in [t_0, t_1]
\end{align*}
If $C = 0$, then $u = 0$ for $t \in [t_0, t_1]$.
\begin{proof}
\begin{enumerate}
\item We first consider the case where $C > 0$. Let
\[
v(t) = C + \int_{t_0}^t g(s) u(s) ds
\]
Then $u(t) \leq v(t)$ on $[t_0, t_1]$ (by assumption), and $v(t) \leq C > 0$ on $[t_0, t_1]$ (since $g, u \geq 0$). Differentiating $v$ with respect to $t$, we have
\[
v'(t) = g(t) u(t) \leq g(t)v(t)
\] 
Since $v(t) > 0$ on $[t_0, t_1]$, we can divide by it to get
\begin{align*}
\dfrac{\dot{v}(t)}{v(t)} &\leq g(t) && t \in [t_0, t_1]
\end{align*}
Now, we integrate $g(s)$.
\begin{align*}
\int_{t_0}^t g(s) ds &\geq \int_{t_0}^t \dfrac{\dot{v}(s)}{v(s)} ds \\
&= \int_{t_0}^t \frac{d}{ds}\left( \log {v(s)} \right) ds \\
&= \log\dfrac{v(t)}{v(t_0)}\\
&= \log\dfrac{v(t)}{C}
\end{align*}
Exponentiate both sides and multiply by $C$ to get
\begin{align*}
u(t) &\leq C \exp \left( \int_{t_0}^t g(s) ds \right) && t \in [t_0, t_1]
\end{align*}
\item If $C = 0$, then for $\epsilon > 0$,
\begin{align*}
u(t) \leq \epsilon + \int_{t_0}^t g(s) u(s) ds && t \in [t_0, t_1]
\end{align*}
By step (i), for $t \in [t_0, t_1]$ 
\begin{align*}
u(t) &\leq \epsilon \exp \left( \int_{t_0}^t g(s) ds \right) \\&\leq \epsilon \exp \left( \int_{t_0}^{t_1} g(s) ds \right) \\
&\leq M \epsilon
\end{align*}
which is independent of $t$. Since $\epsilon$ is arbitrary, we conclude that $u = 0$ on $[t_0, t_1]$.
\end{enumerate}
\end{proof}
\end{lemma}

We can now show that $\dot{u} = A(t) u$ has a solution for all $t$ and all initial conditions $u_0$.

\begin{theorem}
Consider the system
\begin{align*}
\dot{u} &= A(t) u \\
u(s) &= u_0
\end{align*}
where $u \in \R^n$ and $A:\R \rightarrow \R^{n \times n} \in C^0$. Then there exists a unique $C^1$ function $\Phi: \R \times \R \rightarrow \R^n$, called the \emph{fundamental matrix solution}, such that for all $t, s, r \in \R$
\begin{enumerate}[(i)]
\item $\Phi(t, t) = I$ \\
\item $\Phi(t, r)\Phi(r, s) = \Phi(t,s)$
\item $u(t) = \Phi(t, s)u_0$ is the unique solution to the system with initial condition $u(s) = u_0$.
\end{enumerate}
\begin{proof}
\begin{enumerate}
\item Writing the ODE as $\dot{u} = f(u, t)$ with $f(u,t) = A(t) u$, $f(u, t)$ is locally Lipschitz in $u$ for each fixed $t$ (since it is linear in $u$ in that case.) Thus the initial value problem has a unique solution $u(t)$ for $t$ near $s$. 
\item We wish to show that $u(t)$ exists for all $t \in \R$. Suppose this solution exists on $[s, t_0)$, but does not exist beyond $t_0$. Then we know $u(t)$ must blow up as it approaches $t_0$. We will show this cannot happen.
\item We integrate the ODE to get
\begin{align*}
u(t) &= u_0 + \int_s^t A(\tau)u(\tau)d\tau && t \in [s, t_0)
\end{align*}
Taking absolute values and using the operator norm of $A(t)$, we get
\begin{align*}
|u(t)| &= |u_0| + \int_s^t ||A(\tau)||\:|u(\tau)|d\tau && t \in [s, t_0)
\end{align*}
\item This satisfies the hypotheses of Gronwall's Inequality. Thus, for $t \in [s, t_0)$
\begin{align*}
|u(t)| &\leq |u_0| \exp \left( \int_s^t ||A(\tau)|| d\tau \right) \\
&\leq |u_0| \exp \left( \int_s^{t_0} ||A(\tau)|| d\tau \right) \\.&\leq C |u_0|
\end{align*}
where the bound holds uniformly for $t \in [s, t_0)$ since it does not depend on $t$.
\item We conclude that $u(t)$ cannot blow up as $t \rightarrow t_0$, thus $u(t)$ must exist for all $t > s$. A similar argument shows that $u(t)$ must exist for all $t < s$.
\item Thus we can define $u(t) = \Phi(t, s)u_0$ to be the unique solution with initial condition $u(s) = u_0$. The properties of $\Phi(t,s)$ follow from the linearity of the system. $\Phi(t,s)$ is a matrix, and the $k$-th column of $\Phi(t,s)$ is the unique solution at time $t$ resulting from an initial condition $e_k$ at time $s$, where $e_k$ is the $k-$th standard unit vector in $\R^n$. 
\end{enumerate}
\end{proof}
\end{theorem}

We collect some properties of $\Phi(t,s)$ in the following lemma. We first give the following definition of a matrix solution.

\begin{definition}A \emph{matrix solution} of the ODE $\dot{u} = A(t)u, u \in \R^n$ is a $n \times n$ matrix $\Psi(t)$ whose columns are linearly independent solutions.
\end{definition}

Since the ODE is linear, matrix solutions are well defined, i.e. if the columns are linearly independent for some $t$ then they are linearly independent for all $t$.

\begin{lemma}
Let $\Phi(t, s)$ be the fundamental matrix solution for $\dot{u} = A(t) u$. Then
\begin{enumerate}[(i)]
\item $||\Phi(t,s)|| \leq \exp\left( \int_s^t ||A(\tau)|| d\tau \right)$
\item $\Phi(t,s)^{-1} = \Phi(s,t)$ \\
\item If we define $\Phi(t) = \Phi(t, 0)$, then $\Phi(t,s) = \Phi(t)\Phi(s)^{-1}$
\item Let $\Psi(t)$ be any $n\times n$ matrix solution of $\dot{u} = A(t) u$. Then 
	\begin{enumerate}[(a)]
	\item $\Psi(t) = \Phi(t) \Psi(0)$
	\item Liouville's formula states that
	\begin{equation}
	\det \Psi(t) = \det \Psi(s) \exp\left( \int_s^t \text{Tr }A(\tau) d \tau \right)
	\end{equation}
	\end{enumerate}
\end{enumerate}
\begin{proof}
(i) follows from the use of Gronwall's inequality in the previous theorem. (ii) follows since $\Phi(s,t)\Phi(t,s) = \Phi(t,s)\Phi(s,t) = I$ by uniqueness of solutions. (iii) follows from $\Phi(t,s) = \Phi(t,0)\Phi(0,s) = \Phi(t)\Phi(s)^{-1}$. (iv)(a) follows from the definition of the fundamental matrix solution $\Phi$.\\

For Liouville's (iv), we do the following.
\begin{enumerate}
\item Since $\Psi(t)$ is a matrix solution, its columns are linearly independent, so $\det \Psi(s) \neq 0$ for all $s$.
\item Take the matrix $\Psi(t)\Psi(s)^{-1}$ representing the flow of the system from $s$ to $t$, and write it in column form as
\[
\Psi(t)\Psi(s)^{-1} = \left[\Psi(t)\Psi(s)^{-1} e_1, \dots \Psi(t)\Psi(s)^{-1}e_n\right]
\]
\item Now take the derivative of $\det \Psi(t)\Psi(s)^{-1}$. Since the determinant is linear in each column (and row), the derivative of the determinant is the sum of the determinants where the derivative is applied to each column.
\begin{align*}
\frac{d}{dt} \det \Psi(t)\Psi(s)^{-1} &= \sum_j \det \left[\Psi(t)\Psi(s)^{-1} e_1, \dots \frac{d}{dt}\Psi(t)\Psi(s)^{-1}e_j, \dots, \Psi(t)\Psi(s)^{-1}e_n\right] \\
&= \sum_j \det \left[\Psi(t)\Psi(s)^{-1} e_1, \dots A(t) \Psi(t)\Psi(s)^{-1}e_j, \dots, \Psi(t)\Psi(s)^{-1}e_n\right]
\end{align*}
where we used the fact that $\Psi(t)\Psi(s)^{-1}$ satisfies the ODE.
\item Evaluate this at $t = s$ to get
\begin{align*}
\frac{d}{dt} \det \Psi(t)\Psi(s)^{-1}\Big|_{t = s} &= \sum_j \det \left[e_1, \dots, A(s) e_j, \dots, e_n\right] \\
&= \sum_j [A(s)]_{jj} \\
&= \text{Tr }A(s)
\end{align*}
\item Thus since $\Psi(s)$ is invertible, i.e. has nonzero determinant
\begin{align*}
\text{Tr }A(s) &= \frac{d}{dt} \det \Psi(t)\Psi(s)^{-1}\Big|_{t = s} \\
&= \frac{d}{dt} \det \Psi(t) \det(\Psi(s)^{-1}) \Big|_{t = s} \\ 
&= \frac{d}{dt} \det \Psi(t) \det(\Psi(s))^{-1} \Big|_{t = s} \\
\text{Tr }A(s) \det(\Psi(s)) &= \frac{d}{dt} \det \Psi(t)\Big|_{t = s}
\end{align*}
Since we are taking $s = t$, this becomes
\begin{align*}
\frac{d}{dt} \det(\Psi(t)) &= \text{Tr }A(t) \det(\Psi(t))
\end{align*}
\item Integrate this with initial condition at $s$ o get Liouville's formula.
\end{enumerate} 
\end{proof}
\end{lemma}

Now that we have shown that the fundamental solution exists for linear ODEs of the form $\dot{u} = A(t) u$, we will show that a unique solution exists for the inhomogeneous ODE $\dot{t} = A(t)u + g(u, t)$. Even better, we have a formula for the solution!

\begin{theorem}[Variation of Constants Formulas]\label{VOC}
\begin{enumerate}[(i)]
\item Consider the ODE
\begin{align*}
\dot{u} &= A(t)u + g(t) \\
u(s) &= u_0
\end{align*}
with $A(t)$ and $h(t)$ continuous. Then the unique solution, which is valid for all $t \in \R$ is given by
\begin{equation}\label{VOC1}
u(t) = \Phi(t,s)u_0 + \int_s^t \Phi(t, \tau) g(\tau) d \tau
\end{equation}
\item Consider instead the ODE
\begin{align*}
\dot{u} &= A(t)u + g(u, t) \\
u(s) &= u_0
\end{align*}
where this time $g$ depends on $u$ as well. Suppose $g$ is locally Lipschitz, and let $u(t)$ be the unique solution on an interval $I$ containing $s$. Then for all $t \in I$, $u(t)$ can be written as
\begin{equation}\label{VOC2}
u(t) = \Phi(t,s)u_0 + \int_s^t \Phi(t, \tau) g(u(\tau), \tau) d \tau
\end{equation}
Note that this is not really a formula for the solution $u(t)$ since $u$ appears on both sides.
\end{enumerate}
\begin{proof}
For (i), you can be clever and derive the formula, but honestly it is easiest just to plug it in and verify that it satisfies the ODE. Since $||\Phi(t,\tau)|| \leq \exp\left( \int_s^t ||A(z)|| dz \right)$ by Gronwall's Inequality and $h(\tau)$ is bounded on $[s, t]$ by continuity, the integral always has finite magnitude, thus the solution does not blow up and is defined for all $t$.\\

Similarly, you can verify (ii) by plugging in the formula.
\end{proof}
\end{theorem}

Finally, we look at the variational equation, which shows up all the time in applications.

Consider the ODE
\[
\dot{u} = f(u)
\]
with $u \in \R^n$ and $f \in C^1$. Let $\Phi_t(u)$ be the flow of the system. Since $f$ does not depend on $t$, this flow is independent of what time we use for our initial condition, so we can always take it at $t = 0$. Thus, for all $t \in \R$, $u \in \R^n$ we have

\begin{align*}
\frac{d}{dt}\Phi_t(u) &= f(\Phi_t(u)) \\
\Phi_0(u) &= u
\end{align*}

Now we differentiate this in $u$ at $u = u_0$.

\begin{align*}
\frac{d}{dt} D\Phi_t(u_0) &= Df(\Phi_t(u_0)) D\Phi_t(u_0)
\end{align*}

\subsection{Floquet Theory}

Floquet theory is a tool used to analyze linear ODEs $\dot{u} = A(t) u$ the case where the matrix $A(t)$ is periodic. For motivation, consider the system

\begin{align*}
\dot{u} &= f(u) \\
u(0) &= u_0
\end{align*}

where $f$ is smooth, and suppose we have a solution $q(t)$ which is a periodic orbit, i.e. $q(t + T) = q(t)$ for all $t$. Linearizing about this solution, we obtain the linear ODE

\[
\dot{v}(t) = DF(q(t)) v(t)
\]

Since $f$ does not depend on $t$, the dependence of $DF(q(t))$ on $t$ will be via $q(t)$ and its derivatives, all of which are periodic with period $T$. Thus $DF(q(t))$ will be periodic with period $T$. \\

With that example in mind, we will consider the linear system

\begin{align*}
\dot{u} &= A(t) u
\end{align*}

where $u \in \R^n$, $A(t)$ is a continuous $n \times n$ matrix which is periodic with period $T$. The Floquet theorem lets us decompose the fundamental matrix solution $\Phi(t)$ into the product of a periodic matrix and a matrix exponential.

\begin{theorem}[Floquet]
Let $\Phi(t)$ be the fundamental matrix solution to $\dot{u} = A(t) u$, where $A(t)$ is continuous and $T-$periodic. Then
\begin{enumerate}[(i)]
\item There exists a matrix $B$ (which may be complex) and a $T-$periodic matrix $P(t)$ (which may be complex valued) with $P(0) = I$ such that 
\[
\Phi(t) = R(t)e^{Bt}
\]
\item There exists a real matrix $R$ and a real-valued $2T-$periodic matrix $Q(t)$ with $Q(0) = I$ such that
\[
\Phi(t) = Q(t)e^{Rt}
\]
\end{enumerate}
\begin{proof}
We first prove (i)
\begin{enumerate}
\item First, we show that $\Phi(t+T)$ is a matrix solution for the system.
\begin{align*}
\frac{d}{dt}\Phi(t+T) &= A(t+T) \\
&= A(t)\Phi(t+T)
\end{align*}
since $A(t)$ is $T$-periodic. Since $\Phi(t)$ is a flow, we also have $\Phi(t+T) = \Phi(t)\Phi(T)$.
\item Let $B$ be any complex-valued matrix, and define
\[
P(t) = \Phi(t) e^{-Bt}
\]
This definition implies that $P(0) = I$. Then we have
\begin{align*}
P(t+T) &= \Phi(t+T) e^{-B(t+T)} \\
&= \Phi(t)\Phi(T) e^{-B T} e^{-B t}
\end{align*}
If we can find a matrix $B$ so that $\Phi(T) e^{-B T} = I$, i.e. $e^{BT} = \Phi{T}$, then 
\begin{align*}
P(t+T) &= \Phi(t) e^{-B t} = P(t)
\end{align*}
Thus $P$ is $T-$periodic and $\Phi(t) = P(t) e^{Bt}$.
\item We have reduced the problem to finding such a $B$. First, we note that $\Phi(T)$ is invertible, since it is a fundamental matrix solution. Next, we transform $\Phi(T)$ into Jordan canonical form $J$ via an invertible matrix $S$, so we reduce our problem to 
\begin{equation}
J = S^{-1}\Phi(T)S = S^{-1}e^{BT}S 
= e^{S^{-1} B S T} = e^{C}
\end{equation}
Thus, we only have to solve $e^{CT} = J$ for a complex matrix $C$, where $J$ has no eigenvalues of 0 since $\Phi(T)$ is invertible. 
\item Since $J$ is a block diagonal matrix composed of Jordan blocks, since the matrix exponential acts on each Jordan block independently, it suffices to consider a single $k \times k$ Jordan block $J$. This Jordan block has the form $J = \lambda I + N$, where $\lambda \neq 0$ and $N$ is the standard nilpotent matrix consisting of ones on the first upper diagonal. Since $\lambda \neq 0$, we write this as
\[
J = \lambda\left(I + \frac{1}{\lambda}N\right)
\]
Thus we only need to solve
\[
e^{C} = \lambda\left(I + \frac{1}{\lambda}N\right)
\]
for a $k \times k$ matrix $C$.
\item Next, we get rid of the $\lambda$ out front. Let $\tilde{C} = (\log \lambda)I + \tilde{C}$. Substituting this in, and noting that $\log \lambda)I$ and $\tilde{C}$ commute, we have
\begin{align*}
e^{(\log \lambda)I + \tilde{C}} &= \lambda\left(I + \frac{1}{\lambda}N\right) \\
e^{(\log \lambda)I} e^{\tilde{C}} &= \lambda\left(I + \frac{1}{\lambda}N\right) \\
\lambda I e^{\tilde C} &= \lambda\left(I + \frac{1}{\lambda}N\right) \\
e^{\tilde C} &= \left(I + \frac{1}{\lambda}N\right) \\
\end{align*}
Thus, we only need to solve 
\[
e^{\tilde C} = \left(I + \frac{1}{\lambda}N\right)
\]
for a $k \times k$ matrix $\tilde{C}$. We solve this by taking the ``matrix logarithm''. The formal Taylor series for $\log(I + M)$ is
\[
\log(I + M) = \sum_{n=1}^\infty (-1)^{n+1} \frac{M^n}{n}
\]
where this is only formal since there is no guarantee this will converge unless $||M|| < 1$. If this series does converge, however, it follows that
\[
e^{\log(I+M)} = I + M
\]
There is one more case where the series is guaranteed to converge, which happens in our case. The formal Taylor series for $\log(I + \frac{1}{\lambda}N)$ converges since $N$ is nilpotent ($N^k = 0$), thus the sum is actually a finite sum! Thus we conclude that 
\[
\tilde{C} = \log\left( I + \frac{1}{\lambda}N \right) = \sum_{n=1}^{k-1} \frac{(-1)^{n+1}}{n} \left( \frac{1}{\lambda}N \right)^n
\]
\end{enumerate}

For part (ii), we note that

\[
\Phi(t + 2T) = \Phi(t)\Phi(T)^2
\]

By a similar argument we can show that there exists a real matrix $R$ such that $e^{2RT} = \Phi(T)^2$. Letting $Q(t) = \Phi(t)e^{-Rt}$, it follows thtat

\begin{align*}
Q(t+2T) &= \Phi(t+2T)e^{-2RT}e^{-Rt} \\
&= \Phi(t)\Phi(T)^2 e^{-2RT} e^{-Rt} \\
&= \Phi(t) e^{-Rt} \\
&= Q(t)
\end{align*}

thus $Q(t)$ is $2T-$periodic, everything is real, and we can write

\[
\Phi(t) = Q(t) e^{Rt}
\]

\end{proof}
\end{theorem}

The representation $\Phi(t) = P(t)e^{Bt}$ from the Floquet Theorem is called the \emph{Floquet normal form} for the fundamental matrix solution $\Phi(t)$. We use this normal form to examine the stability of the equilibrium at 0 in the $T-$periodic linear system $\dot{u} = A(t) u$. If this system comes from the linearization about a periodic orbit, this then tells us about the linear stability of that periodic orbit.\\

Take an initial condition $u_0$ at $t = 0$, and let $u(t)$ be the unique solution with $u(0) = u_0$. Then every time we move forward by $T$, we apply the map $\Phi(T)$, i.e.

\begin{align*}
u(T) &= \Phi(T) u_0 \\
u(2T) &= \Phi(T)^2 u_0 \\
&\vdots
\end{align*}

The operator $\Phi(T)$ is known as the \emph{monodromy matrix} or \emph{monodromy operator}. By the Floquet theorem, this operator can be written as

\[
M = \Phi(T) = P(T)e^{BT} = P(0) e^{BT} = e^{BT}
\]

since $P$ is $T-$periodic, with $P(0) = I$. Thus we can study stability of the equilibrim at 0 by looking at the discrete dynamical system

\[
v_{n+1} = M v_n
\]

where $M = e^{BT}$ is a constant matrix. From the theory of discrete dynamical systems, the stability of 0 depends on the eigenvalues of $M$, which are called the \emph{Floquet multipliers}. If $|\rho| < 1$ for all Floquet multipliers $\rho$ of $M$, then 0 is stable (in the discrete system), thus 0 is stable (in the original system), and so $||\Phi(t)|| \rightarrow 0$ as $t \rightarrow \infty$.\\

The eigenvalues of $B$ are known as the \emph{Floquet exponents}. The stability criterion can also be stated in terms of the Floquet exponents: If $\text{Re }\lambda < 0$ for all Floquet exponents $\lambda$, then $||\Phi(t)|| \rightarrow 0$ as $t \rightarrow \infty$.\\

Finally, we remark that since the Floquet multipliers are eigenvalues of a matrix $M$, they are unique. The Floquet exponents are not unique, since we can derive another Floquet normal form for the system.

\begin{align*}
\Phi(t) &= R(t) e^{Bt}  \\
&= R(t) e^{-2\pi i t/T}e^{2\pi i t/T}e^{Bt} \\
&= [ R(t) e^{-2\pi i t/T} ] e^{[B + (2 \pi i/T) I]t}
\end{align*}

$\tilde{R}(t) = R(t) e^{-2\pi i t/T}$ is still $T-$periodic, thus this is a valid Floquet normal form. We have a new matrix $\tilde{B} = B + (2 \pi i/T) I$, whose eigenvalues are shifted by $2 \pi i/T$. Thus Floquet exponents are only unique modulo $2 \pi i/T$.


\end{document}