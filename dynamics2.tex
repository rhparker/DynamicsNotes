\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont}
\usepackage[shortlabels]{enumitem} 
\newcounter{question}
\setcounter{question}{0}
\usepackage{parskip}
\setlength{\parindent}{0pt}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{ esint }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{range}

\title{Dynamics Notes 2}
\author{Ross Parker}

\begin{document}

\section{Dynamical Systems}

Consider the dynamical system on $\R^n$

\begin{align*}
\dot{u} &= f(u) \\
u(0) &= u_0
\end{align*}

where $f: \R^n \rightarrow \R^n$ is sufficiently smooth (at least $C^1$ for our purposes). Assume that solutions exist for all ICs $u_0$ and all $t \in \R$ (we will address the issue of global existence at a later point).\\

We define the flow of the dynamical system as follows.

\pagebreak

\subsection{Lyapunov Functions}

Recall the definitions of stabiltiy for an equilibrium point $u^*$ of a dynamical system.

\begin{enumerate}
\item An equilibrium point $u^*$ is \emph{stable} if trajectories that start close stay close. In precise mathematical terms, for every $\epsilon > 0$ there exists $\delta > 0$ such that for every initial condition $u_0 \in B(u_0, \delta)$, the trajectory $\Phi_t(u_0) \in B(u_0, \epsilon)$ for all $t \geq 0$. 
\item An equilibrium point $u^*$ is \emph{asymptotically stable} if it is stable and trajectories that start close approach it as $t \rightarrow \infty$. In other words, for every $\epsilon > 0$ there exists $\delta > 0$ such that (i) the stability definition applies; and (ii) for every initial condition $u_0 \in B(u_0, \delta)$, $d(\Phi_t(u_0), u^*) \rightarrow 0$ as $t \rightarrow \infty$.
\end{enumerate}

In general, there is no good way to determine if an equilibrium point is stable or asymptotically stable. However, we have a few tools which can do the job. One such tool is the Lyapunov function. The idea is that we can determine the behavior of trajectories near an equilibrium point without actually solving the governing ODE.\\

The idea is as follows. Consider a flow $\phi_t$ on the plane. (This works in higher dimensions as well, but I cannot visualize things in more than three dimensions.) Imagine that the flow represents a ball rolling around the plane. The dynamical system gives the rules that tell us how the ball rolls. Now suppose we associate an energy with every point in the plane, so now we have a three-dimensional energy landscape. Suppose the rules now dictate that the ball must always flow downhill in this energy landscape. Then any local minimum, or ``well'', of the energy must be an asymptotically stable equilibrium point. There is no escaping the well.\\

Now we make this idea mathematically precise. Consider the autonomous dynamical system

\begin{align}\label{LyODE}
\dot{u} &= f(u) && u \in \R^n, f \in C^1
\end{align}

We need the system to be autonomous so that the energy landscape remains constant in $t$. Let $u_0$ be an equilibrium point for \eqref{LyODE}, i.e. $f(u_0) = 0$. Then we define a Lyapunov function as follows. Think of the Lyapunov function as energy.\\

\begin{definition}
A $C^1$ function $V: U \subset \R^n \rightarrow \R$, where $U$ is an open set containing $u^*$ is a \emph{Lyapunov function} for \eqref{LyODE} at the equilibrium point $u^*$ if
\begin{enumerate}[(i)]
\item $V(u_0) = 0$
\item $V(u) > 0$ for $u \neq u_0$
\item $\dot{V} = \langle \nabla V(u(t)), \dot{u} \rangle = \langle \nabla V(u(x)), f(u) \rangle \leq 0$ 
(energy cannot increase with time)
\end{enumerate}
The function $V$ is a \emph{strict Lyapunov funcion} if, in addition,
\begin{enumerate}[(i)]
\setcounter{enumi}{3}
\item $\dot{V} < 0$ for $u \neq u_0$ (energy strictly decreases with time)
\end{enumerate}
\end{definition} 

We have the following theorem.

% Theorem : Lyapunov Stability

\begin{theorem}[Lyapunov Stability Theorem]
If there is Lyapunov function $V$ defined in an open neighborhood of an equilibrium point $u^*$ of \eqref{LyODE}, then $u^*$ is stable. If $V$ is a strict Lyapunov function, then $u^*$ is asymptotically stable.
\end{theorem}
\begin{proof}
The idea of the proof is as follows. For simplicity, suppose we are in the plane. Suppose the levels sets of $V$ are concentric rings around $u^*$. Take our initial condition $u_0$ on one of the rings, e.g. on the level set $\{V(u) = c\}$. Then the condition $\dot{V} \leq 0$ means that the trajectory starting at $u_0$ either stays on the ring $\{V(u) = c\}$ or moves to a more inner ring, thus $u^*$ is stable. The condition $\dot{V} < 0$ means the trajectory must always move to a more inner ring as time increases, thus $u^*$ is asymptotically stable. Of course, the level sets of $V$ are not, in general, concentric rings, so we will have to be more careful. The proof below is adapted from Chicone (2006).

\begin{enumerate}
	\item Let $\epsilon > 0$ be sufficiently small so that $B(u^*, \epsilon) \subset U$, where $U \subset \R^n$ is the domain of the Lyapunov function $V$.

	\item Consider $\partial B(u^*, \epsilon)$, the boundary of the $\epsilon-$ball around $u^*$. Since $\partial B(u^*, \epsilon)$ is compact, $V$ must attain an absolute minimum $M > 0$ at a point $u_M$ on it, and this minimum cannot be 0 since, by the definition of a Lyapunov function, $V(u) > 0$ everywhere on $U$ except for $u^*$. Thus $V(u) \geq M$ on $\partial B(u^*, \epsilon)$. 

	\item Since $V$ is continuous and $V(u^*) = 0$, we can find $\delta > 0$ such that $V(u) \leq M/2$ for $u \in \overline{B}(u^*, \delta)$. Note that we must have $\delta < \epsilon$, since otherwise the point $u_M$ would be in $\overline{B}(u^*, \delta)$, and $V(u_M) = M$.

	\item Let $\Phi_t$ denote the flow of $\eqref{LyODE}$. Take an initial condition $u_0 \in B(u^*, \delta)$. Then from the definition of a Lyapunov function, 

	\begin{align*}
	\frac{d}{dt} V(\Phi_t(u_0)) 
	&= \langle \nabla V(\Phi_t(u_0)), \frac{d}{dt} \Phi_t(u_0) \rangle \\
	&= \langle \nabla V(\Phi_t(u_0)), f(\Phi_t(u_0)) \rangle \\
	&\leq 0
	\end{align*}

	Thus the function $V(\Phi_t(u_0))$ is nondecreasing in $t$. Since $V(\Phi_0(u_0)) = V(u_0) \leq M/2 < M$ for all $u_0 \in B(u^*, \delta)$, it must be the case that $V(\Phi_t(u_0)) \leq M/2 < M$ for all $t \geq 0$, so long at the flow $\Phi_t(u_0)$ is defined. 

	\item Next, we show that, as long at it is defined, the solution $\Phi_t(u_0)$ must stay within the open ball $B(u^*, \epsilon)$. If not, since trajectories are continuous, $\Phi_T(u_0)$ must hit the ball boundary $\partial B(u^*, \epsilon)$ at some time $T > 0$. Since $M$ is the minimum value of $V$ on this ball, we have $V( \Phi_T(u_0) ) \geq M$, which contradicts what we showed above.

	\item Finally, we show that the flow $\Phi_t(u_0)$ is defined for all $t \geq 0$. By the Extension Theorem, $\Phi_t(u_0)$ can fail to exist for some $t > 0$ only in two ways: (i) the solution $\Phi_t(u_0)$ blows up in finite time, or (ii) the solution $\Phi_t(u_0)$ approaches the boundary of definition of $f$. (i) cannot happen, since we showed that all solutions are trapped inside $B(u^*, \epsilon)$. (ii) cannot happen, since $f$ is defined for all $u \in \R^n$.
	
	\item We have shown that for all ICs $u_0 \in B(u^*, \delta)$, $\Phi_t(u_0) \in B(u^*, \epsilon)$ for all $t > 0$. Thus $u^*$ is stable.

	\item Finally, we show that if $V$ is a strict Lyapunov function, $u^*$ is asymptotically stable. Suppose that $u^*$ is not asymptotically stable. Then we can find an initial condition $u_0 \in B(u^*, \delta)$ such that $\Phi_t(u_0)$ does not converge to $u^*$. Take any sequence of increasing times $0 \leq t_1 < t_2 < \cdots$ with $t_k \rightarrow \infty$, and note that all sequences $\{ \Phi_{t_k} u_0 \}$ are contained in the compact set $B(u^*, \epsilon)$. Since $\Phi_t(u_0)$ does not converge to $u^*$, we can find such a sequence of times so that $\Phi_{t_k} u_0 \rightarrow \tilde{u} \neq u^*$.

	\item We will show this is impossible. Since $V$ is continuous, $V( \Phi_{t_k}(u_0) ) \rightarrow V(\tilde{u})$ as $k \rightarrow \infty$. Since $u_1 \neq u^*$, the function $V(\Phi_t(u_1))$ is strictly decreasing in $t$ since it is a strict Lyapunov function. This means that $V(\Phi_{t_k}(u_1)) > V(u_1)$ for all $k$. In addition, we have

	\begin{align*}
	\lim_{k\rightarrow \infty}V(\Phi_{1 + t_k}(u_0)) 
	= \lim_{k\rightarrow \infty}V(\Phi_1(\Phi_{t_k}(u_0)))
	= V(\Phi_1(u_1)) < V(u_1)
	\end{align*}

	This means that for all $k \geq K_0$, $V(\Phi_{1 + t_k}(u_0)) < V(u_1)$. But since $t_k \rightarrow \infty$, we can find $K \in \N$ with $t_K \geq k_0$. Thus we have $V(\Phi_{t_K}(u_1)) < V(u_1)$, which is a contradiction. We conclude that $u^*$ must be asymptotically stable.

\end{enumerate}
\end{proof}

The take-home message is that if we have a Lyapunov function, there is a lot we can say about stability of an equilibrium point. The trick, however, is finding such a function in the first place. The unfortunate reality is that there is no tried-and-true method to find a Lyapunov function for a given system; it often is little more than (hopefully educated) guesswork. There are a few guidelines we can use, which we will discuss in the following examples.

\begin{enumerate}

\item Consider the two-dimensional ODE 

\begin{align*}
\dot{x} &= -xy^2 \\
\dot{y} &= 3yx^2
\end{align*}

The origin is an equilibrium point, but the Jacobian matrix for the origin is the zero matrix, so Hartman-Grobman is of no use. Looking for a Lyapunov function seems like a good idea. A simple quadratic form is a good guess, since it is positive everywhere except for the origin. Let's try

\[
V(x, y) = 3x^2 + y^2
\]

The total derivative is 

\begin{align*}
\dot{V}(t) &= \langle DV(x, y), (x y^2, 3 y x^2) \rangle \\ 
&= \langle DV(x, y), (-x y^2, 3 y x^2) \rangle \\
&= \langle (6x, 2y), (-x y^2, 3 y x^2) \rangle \\
&= 6 x^2 y^2 - 6 x^2 y^2 \\
&= 0
\end{align*}

Thus the origin is stable.

\item Another way to find Lyapunov functions is to use conservation of energy. Consider the undamped harmonic oscillator $\ddot{x} + \omega^2 x = 0$ for $\omega > 0$, which we can write as the first-order system

\begin{align*}
\dot{x} &= y \\
\dot{y} &= -\omega^2 x
\end{align*}

The origin is an equlibrium point. Since the equation is linear, we have eigenvalues $\pm \sqrt{-\omega^2} = \pm i \omega$ for the Jacobian at the origin, which is a linear center. If we want to use a Lyapunov function, a natural choice is the energy of the system, which in this case is the sum of kinetic and elastic potential energy.

\[
E(x, y) = \frac{1}{2}y^2 + \frac{1}{2}\omega x^2
\] 

The origin is a zero of the energy, and it is conserved along trajectories of the system, since

\begin{align*}
\dot{V}(t) &= \langle (\omega^2 x, y), (y, -\omega^2 x) \rangle \\ 
&= 0
\end{align*}

\item We can generalize this to gradient systems. Consider the system

\[
\dot{u} = -\nabla V(u)
\]

where $u \in \R^n$ and the potential function $V:\R^n \rightarrow \R \in C^2$. Then if a point $u_0$ is a strict local minimum of the potential, it is an equlibrium point and is stable.\\

To this, note that since $u_0$ is a local minimum of $V$, this implies $\nabla V(u_0) = 0$, thus $u_0$ is an equilbrium point. Shift the potential $V$ to so that $V(u_0) = 0$ (which does not affect the system at all). Since

\[
\dot{V}(u) = \langle \nabla V(r), -\nabla V(u) \rangle 
= -|\nabla V(u)|^2
\]

$\dot{V}(u_0) = 0$ and $\dot{V}(u) < 0$ near $u_0$ since $u_0$ is a local minimum of $V$. Thus $V$ is a strict Lyapunov function near $u_0$, and we conclude that $u_0$ is asymptotically stable.

\item The more usual situation in physics is the force is the gradient of a potential $V$. Writing down Newton's laws for this situation an assuming the mass $m = 1$, we have for $u \in R^n$

\[
\ddot{u} = -\nabla V(u)
\]

As a first-order system, this becomes

\begin{align*}
\dot{u} &= v \\
\dot{v} &= -\nabla V(u)
\end{align*}

Suppose that $u_0$ is a strict local minimum of the potential $V$, and without loss of generality take $V(u_0) = 0$. Then $(u, v) = (u_0, 0)$ is an equilbrium point of the system. The energy is given by the sum of kinetic and potential energy, which is

\[
E(u, v) = \frac{1}{2} v^2 + V(u)
\]
The energy is conserved, since

\begin{align*}
\dot{E}(u, v) &= \langle (\nabla V(u), v), (v, -\nabla V(u) ) \rangle = 0
\end{align*}

Thus the energy $E(u, v)$ is conserved, and so $u_0$ is stable.

\end{enumerate}

\subsection{Poincare-Bendixson Theorem}

This important theorem applies only to differential equations in the plane and gives a criterion for the existence of a periodic orbit. It only works in the plane. The rough intuition why is that if a trajectory is ``stuck in a box'', there is no way for it to get out.\\

For a dyamical system with flow $\Phi_t$, recall the following definitions.

\begin{definition}The \emph{orbit} of an initial condition $u_0$, denoted $\gamma(u_0)$, is the set of all points in the trajectory of the dynamical system passing through $u_0$. In other words,
\[
\gamma(u_0) = \{ \Phi_t(u_0), t \in \R \}
\]
Similarly, we can define the \emph{forward orbit} and \emph{backward orbit} of an initial condition.
\begin{align*}
\gamma^+(u_0) &= \{ \Phi_t(u_0), t \geq 0 \} \\
\gamma^-(u_0) &= \{ \Phi_t(u_0), t \leq 0 \}
\end{align*}
\end{definition}

We also recall the definition of the $\omega-$limit set.
\begin{definition}The \emph{$\omega-$limit set} of an initial condition $u_0$, denoted $\omega(u_0)$, is the set of all points $u$ for which the forward trajectory starting at $u_0$ gets arbitratily close infinitely often as $t$ increases. Precisely, 
\[
\omega(u_0) = \{ u : \text{there exists an unbounded, increasing sequence of times }\{t_n\} \text{ such that } \Phi_{t_n}(u_0) \rightarrow u \}
\]
\end{definition}

\begin{theorem}[Poincare-Bendixson]
Consider the dynamical system on the plane
\begin{align}\label{planarODE}
\dot{u} = f(u)
\end{align}
If $\gamma^+(u_0)$ is bounded, then exactly one of the following is satisfied.
\begin{enumerate}[(i)]
\item $\omega(u_0)$ is a single periodic orbit.
\item $\omega(u_0)$ is a single fixed point.
\item $\omega(u_0)$ consists of a finite number of fixed points connected by homoclinic and heteroclinic orbits.
\end{enumerate}
\end{theorem}

Before we prove this, we will give a few examples. The key move is to show that the forward orbit starting from some initial condition is bounded. The usual method to do that is to show that the initial condition is contained in a ``trapping region''.

\subsubsection{Poincare-Bendixson Examples}
\begin{example}
\end{example}

\subsubsection{Proof of Poincare-Bendixson}

The result relies on the Jordan Curve theorem, which says that any simple closed curve (i.e. one does not self-intersect) divides the plane into a bounded interior and an unbounded exterior. No additional assumptions are made about the curve. In particular, it does not have to be either smooth or rectifiable. 

\begin{theorem}[Jordan Curve Theorem]A simple closed curve $\Gamma$ divides the plane $\R^2$ into two disjoint, open, connected components: a bounded interior $I$ and an unbounded exterior $E$. The boundary of both regions is $\Gamma$, i.e. $\partial I = \partial E = \Gamma$.
\end{theorem}

Before we continue, we need to discuss coordinate changes of dynamical systems. We have the following lemma.

\begin{lemma}[Change of Coordinates]
Consider the differential equation $\dot{u} = f(u)$ defined on an open set $U$. Let $h: U \rightarrow V$ be a diffeomorphism with inverse $g$. Then $u(t)$ satisfies the original ODE $\dot{u} = f(u)$ if and only if $v(t) = h(u(t))$ satisfies the transformed ODE
\[
\dot{v} = Dg^{-1}(g(v))f(g(v))
\]
\begin{proof}
By the chain rule,
\begin{align*}
\dot{v}(t) &= Dh(u(t))\dot{u}(t) \\
&= Dh(h^{-1}(v(t))) f(u(t)) \\
&= Dh(h^{-1}(v(t))) f(h^{-1}(v(t))) \\
&= Dg^{-1}(g(v(t))) f(g(v(t))) 
\end{align*}
\end{proof}
\end{lemma}

In the next lemma, we devise a specific change of coordinates to ``straighten out'' a vector field in one direction. Although we will only use this lemma in $\R^2$, we will state and prove it in $\R^n$ since it is useful in its own right.

\begin{lemma}[Rectification Lemma, Flow Box Lemma]
Consider the system
\[
\dot{u} = f(u)
\]
for $u \in \R^n$ and $f \in C^1$, and suppose $p$ is not an equilibrium point, i.e. $f(p) \neq 0$. Then there exists a differentiable change of coordinates in a neighborhood $p$ so that, in the new coordinate system, the vector field consists entirely of unit vectors pointing in the same direction.\\

Mathematically, there exists an open neighboorhood of $p$, $\delta > 0$, and a diffeomorphism 

\[
g: (-\delta, \delta) \times B(0, \delta) \subset \R \times \R^{n-1} \rightarrow U
\]

with $u(0, 0) = p$ such that in the new coordinate system, the system becomes

\begin{align*}
\begin{pmatrix}\dot{s}\\ \dot{w} \end{pmatrix} =
\begin{pmatrix}1\\ 0\end{pmatrix}
\end{align*}
where $(s, w) \in \R \times \R^{n-1}$.

\begin{proof}
\begin{enumerate}
\item Since $f(p) \neq 0$, we can extend $\{ f(p) \}$ to a basis $\{ f(p), e_1, \dots, e_{n-1} \}$ of $\R^n$.
\item Define the section $\Sigma$ through the point $p$ as the subspace spanned by $\{e_1, \dots, e_{n-1} \}$ ``attached'' to the point $f(p)$, i.e.
\[
\Sigma = \left\{ p + \sum_{i=1}^{n-1} w_i e_i, w \in \R^{n-1} \right\}
\]
If our basis is orthogonal, this is a $n-1$-dimensional hyperplane passing through $p$ and perpendicular to $f(p)$.

\item Let $\Phi_s$ be the flow of the original system, and define the mapping $g: (-\delta, \delta) \times B(0, \delta) \subset \R \times \R^{n-1} \rightarrow U$ by
\[
g(s, w) = \Phi_s\left( p + \sum_{i=1}^{n-1} w_i e_i \right)
\]
The map $s \mapsto g(s, w)$ is the unique solution to the original ODE with an IC which depends on $w$. We note the following
\begin{align*}
g(0, 0) &= \Phi_0(p) = p \\
\frac{dg}{ds}(0, 0) &= f(p) \neq 0 \\
\frac{dg}{w_i}(0, 0) &= e_i
\end{align*}
where the last one is true since $\Phi_0 = I$. From this, we have $g \in C^1$. Since
\[
Dg(0, 0) = [ f(p) | e_1 | \dots | e_{n-1}]
\]
is invertible (its columns are a basis), we can apply the inverse function theorem. Thus, $g$ is invertible on a neighborhood $(-\delta, \delta) \times B(0, \delta)$ of $(0, 0)$, and $g^{-1}$ is $C^1$.
\item Finally, we verify that we did what we set out to do, i.e. show that we have attained the desired transformation.

FILL THIS IN TOMORROW!!!!!!
\begin{align*}
Dg(g^{-1})
\end{align*}

\end{enumerate}
\end{proof}
\end{lemma}

% consecutive crossing lemma

\begin{lemma}[Consecutive Crossing Lemma]Let $q \in \R^2$ so that $f(q) \neq 0$. Let $\Sigma$ be a section at $q$ which is transverse to $f(q)$, i.e. $\Sigma$ is a line segment which is not parallel to $f(q)$ and through which the vector field $f$ crosses in the same direction as $f$.\\

If for an initial condition $p$ the orbit of $p$ intersects $\Sigma$ for a sequence of increasing times $t_1 < t_2, t_3$ (i.e. $\Phi_{t_1}(p), \Phi_{t_2}(p), \Phi_{t_3}(p) \in \Sigma$), then $\Phi_{t_2}$ lies between $\Phi_{t_1}$ and $\Phi_{t_3}$, i.e. the crossings are consecutive.\\

\begin{proof}

Note that since $f(q) \neq 0$, by continuity of $f$, a section $\Sigma$ always exists. Also note that in order for multiple crossings of the section to be possible, the $\Sigma$ must be bounded.
\begin{enumerate}
\item It suffices to consider the case where $t_2$ is the only time between $t_1$ and $t_3$. (If there are more intermediate intersections, we can start with the three lowest times and keep repeating this to get the result.)

\item If $\Phi_{t_1} = \Phi_{t_2}$, then we have a periodic orbit, and $\Phi_{t_3}$ must be equal to them both. Thus the result holds. From now on, we may therefore take $\Phi_{t_1} \neq \Phi_{t_2}$. For simplicity of description, we will say that $\Phi_{t_2}(p)$ is to the right of $\Phi_{t_2}(p)$ on $\Sigma$. 

\item Consider the set
\[
\Gamma = \{ \Phi_t(p) : t_1 \leq t \leq t_2 \} \cup
[\Phi_{t_1}(p), \Phi_{t_2}(p)]
\]
This is the union of the trajetory between $t_1$ and $t_2$ and piece of the section $\Sigma$ where the trajectory hits at $t_1$ and $t_2$. Since $t_1$ and $t_2$ are consecutive intersections of the trajectory with $\Sigma$ and since trajectories are continuous, $\Gamma$ is a simple, closed (Jordan) curve.

\item By the Jordan Curve Theorem, $\Gamma$ has a bounded interior $I$ and an unbounded exterior $E$. Since trajectories can only cross $\Sigma$ in one direction, $\Phi_t(p)$ must cross $\Sigma$ once at $t_1$, loop around the end of $\Sigma$, and cross it again at $t_2$. Therefore, immediately after $t_2$, the trajectory $\Phi_t(p)$ must in the bounded interior $I$. It can never escape the interior, since that would mean the trajectory would either have to cross itself (forbidden by uniqueness) or cross $\Sigma$ in the wrong direction.

\item The next crossing of $\Sigma$ at time $t_3$ cannot occur to the left of $\Phi_{t_1}(p)$, since it would have to leave $I$ to do that. It cannot occur between $\Phi_{t_1}(p)$ and $\Phi_{t_2}(p)$, since that would mean crossing $\Sigma$ in the wrong direction. The only possibility left is that it must cross to the right of $\Phi_{t_2}(p)$ (by looping back around the end). Thus the three crossings are in consecutive order. 

\end{enumerate}
\end{proof}
\end{lemma}

In the next lemma, we show some stuff.

\begin{lemma}For an initial condition $u_0$, let $u_1 \in \omega(u_0$ with $f(u_1) \neq 0$. Let $\Sigma$ be a section at $u_1$, as described in the previous lemma. Then

\[
\omega(u_0) \cap \Sigma = \{ u_1 \}
\]

\begin{proof}
We prove this by contradiction.
\begin{enumerate}
\item Suppose there is another such intersection points, i.e. we also have $u_2 \in \omega(u_0) \cap \Sigma$, and $u_1 \neq u_2$. Since $u_2 \in \Sigma$, by how we define a section, $f(u_2) \neq 0$. 
\item Choose open, disjoint subintervals $J_1, J_2$ of $\Sigma$ with $u_1 \in J_1$ and $u_2 \in J_2$.
\item Apply the Rectificion Theorem, so that the flow has constant velocity and perpendicular to $\Sigma$.
\item Neither $u_1$ nor $u_2$ are equilibrium points, but both are in $\omega(u_0)$. Thus the trajectory $\Phi_t(u_0)$ must keep passing arbitrarily close to both of them; if the approach is close enough, the trajectory must cross $\Sigma$ when this occurs.
\item Thus, we can find an increasing sequence of times $t_1 < t_2 < t_3$ such that
\begin{align*}
\Phi_{t_1}(u_0) &\in J_1 \\
\Phi_{t_2}(u_0) &\in J_2 \\
\Phi_{t_3}(u_0) &\in J_1
\end{align*}
This violates the Consecutive Crossing Lemma.
\end{enumerate}
\end{proof}
\end{lemma}

We are finally ready to prove the Poincare-Benixson Theorem. 

\begin{enumerate}
\item For a given IC $u_0$, suppose that $\omega(u_0)$ is bounded. Suppose that $\omega(u_0)$ is neither:

\begin{enumerate}[(i)]
\item A single fixed point. In this case, $\omega(u_0)$ is a single equilibrium point $u_1$, and $\omega(u_1) = \{ u_1 \}$.
\item A finite number of fixed points connected by homoclinic and heteroclinic orbits. In this case, $\omega(u_0)$ is a bunch of stuff, but for every $u_1 \in \omega(u_0)$, $\omega{u_1}$ consists entirely of equilibria.
\end{enumerate}
We will show that $\omega(u_0)$ must be a periodic orbit.
\item If neither of those cases is true, then we can find a point $u_1 \in \omega{u_0}$ such that $\omega{u_1}$ does not consist entirely of equilibria; that is, we can find $u_2 \in \omega(u_1)$ such that $u_2$ is not an equilibrium, i.e. $f(u_2) \neq 0$.
\item Let $\Sigma$ be a section at $u_2$. Note that since $u_1 \in \omega(u_0)$, $\omega(u_1) \subset \omega(u_0)$, thus $u_2 \in \omega(u_0)$. Thus by the previous lemma, $\omega(u_0) \cap \Sigma = \{ u_2 \}$.
\item Since $u_2 \in \omega(u_1)$, the trajectory $\Phi_t(u_1)$ must keep passing arbitrarily close to $u_2$, and each time it does so, it must cross $\Sigma$ (by how we constructed the section and the fact that $f(u_2) \neq 0$). Thus we can find an unbounded, increasing sequence of times $t_1 < t_2 < \dots \rightarrow \infty$ such that $\Phi_{t_n}(u_1) \in \Sigma$ for all $n$ and $\Phi_{t_n}(u_1) \rightarrow u_2$ as $n \rightarrow \infty$.
\item Since the $\omega$-limit set is invariant under the flow and $u_1 \in \omega(u_0)$, $\Phi_{t_n}(u_1) \in \omega(u_0)$ for all $n$.
\item By the previous lemma we conclude that $\Phi_{t_n}(u_1) = u_2$ for all $n$, thus $\gamma(u_1)$ is a periodic orbit contained in $\omega(u_0)$ (again by invariance). It is a nontrivial periodic orbit since it contains a point $u_2$ with $f(u_2) \neq 0$).

\item Finally, we show that $\gamma(u_1) = \omega(u_0)$. Since the forward orbit $\gamma^+(u_0)$ is bounded (we finally use our initial assumption here!), $\omega(u_0)$ is connected. Suppose $\omega(u_0) \backslash \gamma(u_1)$ is nonempty. Then the rest of $\omega(u_0)$ must be ``attached to'' the periodic orbit at a point $u^* \in \gamma(u_1)$, i.e. there exists a sequence $u_n^* \in \omega(u_0) \backslash \gamma(u_1)$ such that $u_n^* \rightarrow u^*$. Since $u^*$ is on a periodic orbit, $f(u^*) \neq 0$. Let $\Sigma$ be a section at $u^*$. Then 
\end{enumerate}

\end{document}